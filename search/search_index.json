{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WEBSITE UNDER CONSTRUCTION TODO # Create content Create moaarr content","title":"Home"},{"location":"#todo","text":"Create content Create moaarr content","title":"TODO"},{"location":"3-nodes-swarm/","text":"Docker the Zerg way # For a resilient installation we are going to build a docker swarm of 3 master nodes with also a worker role. (In swarm node can be master or worker - the one running containers - or both) It's important to understand the split brain concept. In the following cluster a quorum have to be kept to assure things don't go sideways. For the quorum to be achieve you should have 2 master nodes up. This is true for Swarm and GlusterFS. Nodes gonna be called: swarm1, swarm2, swarm3 +----------------------+ | +----------------------+ | [Node #1] |10.0.0.51 | | | [Node #1] |10.0.0.52 | | swarm1.lab.local +----------+----------+ swarm2.lab.local | | | | | | +----------------------+ | +----------------------+ | | | +----------------------+ | | [Node #3] |10.0.0.53 | | | swarm3.lab.local +----------+ | | +----------------------+ Why Swarm ? Swarm is an orchestration tool directly provided into docker from the version 1.12. It is easy to use compare to Kubernetes and easier to maintain for a small team. Highly-available (can tolerate the failure of a single component) Scalable (can add resource or capacity as required) Portable (run it on your home today, run it in everywhere tomorrow) Automated (requires minimal care and feeding) Why GlusterFS ? While Docker Swarm is great for keeping containers running and providing scaling capabilities, it does lack direct integration of persistent storage accross nodes. This means if you actually want your containers to keep any data persistent across restarts , you need to provide shared storage to every docker node. This also means you shouldn't use docker volume declaration in you docker files. Installing the host component # Installation is based on a fresh Centos Stream minimal server and should be executed on all nodes.","title":"Architecture"},{"location":"3-nodes-swarm/#docker-the-zerg-way","text":"For a resilient installation we are going to build a docker swarm of 3 master nodes with also a worker role. (In swarm node can be master or worker - the one running containers - or both) It's important to understand the split brain concept. In the following cluster a quorum have to be kept to assure things don't go sideways. For the quorum to be achieve you should have 2 master nodes up. This is true for Swarm and GlusterFS. Nodes gonna be called: swarm1, swarm2, swarm3 +----------------------+ | +----------------------+ | [Node #1] |10.0.0.51 | | | [Node #1] |10.0.0.52 | | swarm1.lab.local +----------+----------+ swarm2.lab.local | | | | | | +----------------------+ | +----------------------+ | | | +----------------------+ | | [Node #3] |10.0.0.53 | | | swarm3.lab.local +----------+ | | +----------------------+ Why Swarm ? Swarm is an orchestration tool directly provided into docker from the version 1.12. It is easy to use compare to Kubernetes and easier to maintain for a small team. Highly-available (can tolerate the failure of a single component) Scalable (can add resource or capacity as required) Portable (run it on your home today, run it in everywhere tomorrow) Automated (requires minimal care and feeding) Why GlusterFS ? While Docker Swarm is great for keeping containers running and providing scaling capabilities, it does lack direct integration of persistent storage accross nodes. This means if you actually want your containers to keep any data persistent across restarts , you need to provide shared storage to every docker node. This also means you shouldn't use docker volume declaration in you docker files.","title":"Docker the Zerg way"},{"location":"3-nodes-swarm/#installing-the-host-component","text":"Installation is based on a fresh Centos Stream minimal server and should be executed on all nodes.","title":"Installing the host component"},{"location":"3-nodes-swarm/DockerSwarm/Installation/","text":"Docker swarm # Installing Docker and Docker compose # Remove runc # # dnf remove runc Adding docker repo # # dnf install -y yum-utils # yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo` # dnf install docker-ce docker-ce-cli containerd.io Installing docker-compose # # curl -L \"https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose # chmod +x /usr/local/bin/docker-compose Let's start the whale # # systemctl enable docker --Now And now create a zerg warm from it # From swarm1: # docker swarm init Swarm initialized: current node (ksdjlqsldjqsd2516685485) is now a manager. To add a worker to this swarm, run the following command: docker swarm \\ join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-814yer1z55vmyk2mwdhvjbob1 \\ 10.0.0.51:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. We are going to add the two other nodes as manager: docker swarm join-token manager To add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-2k0vay9aub5eheikw7qi9v82o 10.0.0.51:2377 Run the command provided on your other nodes to join them to the swarm as managers. After addition of a node, the output of docker node ls (on either host) should reflect all the nodes: # docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION p424u0yvmu0vvc8nsnspv83zw * swarm1.lab.local Ready Active Leader 20.10.6 kg6w6ucpb2jf8v8xqai23pv3a swarm2.lab.local Ready Active Reachable 20.10.6 lam7mgs5wus40iaydvp8u3ss7 swarm3.lab.local Ready Active Reachable 20.10.6 You are now ready to swarm. Official Documentation : https://docs.docker.com/engine/swarm/","title":"Docker swarm"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#docker-swarm","text":"","title":"Docker swarm"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#installing-docker-and-docker-compose","text":"","title":"Installing Docker and Docker compose"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#remove-runc","text":"# dnf remove runc","title":"Remove runc"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#adding-docker-repo","text":"# dnf install -y yum-utils # yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo` # dnf install docker-ce docker-ce-cli containerd.io","title":"Adding docker repo"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#installing-docker-compose","text":"# curl -L \"https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose # chmod +x /usr/local/bin/docker-compose","title":"Installing docker-compose"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#lets-start-the-whale","text":"# systemctl enable docker --Now","title":"Let's start the whale"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#and-now-create-a-zerg-warm-from-it","text":"From swarm1: # docker swarm init Swarm initialized: current node (ksdjlqsldjqsd2516685485) is now a manager. To add a worker to this swarm, run the following command: docker swarm \\ join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-814yer1z55vmyk2mwdhvjbob1 \\ 10.0.0.51:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. We are going to add the two other nodes as manager: docker swarm join-token manager To add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-2k0vay9aub5eheikw7qi9v82o 10.0.0.51:2377 Run the command provided on your other nodes to join them to the swarm as managers. After addition of a node, the output of docker node ls (on either host) should reflect all the nodes: # docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION p424u0yvmu0vvc8nsnspv83zw * swarm1.lab.local Ready Active Leader 20.10.6 kg6w6ucpb2jf8v8xqai23pv3a swarm2.lab.local Ready Active Reachable 20.10.6 lam7mgs5wus40iaydvp8u3ss7 swarm3.lab.local Ready Active Reachable 20.10.6 You are now ready to swarm. Official Documentation : https://docs.docker.com/engine/swarm/","title":"And now create a zerg warm from it"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/","text":"Portainer # Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use. It consists of a single container that can run on any Docker engine (can be deployed as Linux container or a Windows native container, supports other platforms too). Portainer allows you to manage all your Docker resources (containers, images, volumes, networks and more!) It is compatible with the standalone Docker engine and with Docker Swarm mode. Preparaton # Setup Docker Swarm # Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/administration # curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml Customisation # By default portainer stack is using a normal docker volume. So if the node where the container will run fail, then we lost the data. To counter this we are going to use a local file shared accross nodes by GlusterFS. Edit portainer-agent-stack.yml Change: portainer_data:/data for /mnt/docker_data/administration/portainer_data:/data Remove: volumes: portainer_data: The file should look like this (I added some notes as comment): version: '3.2' services: agent: image: portainer/agent volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes networks: - agent_network deploy: mode: global # Will be deploy on all nodes manager or worker accross the swarm. placement: constraints: [node.platform.os == linux] # Will only run on Linux OS. portainer: image: portainer/portainer-ce command: -H tcp://tasks.agent:9001 --tlsskipverify ports: - \"9000:9000\" - \"8000:8000\" volumes: - /mnt/docker_data/administration/portainer_data:/data networks: - agent_network deploy: mode: replicated replicas: 1 # Only one container of this type gonna run at a time. placement: constraints: [node.role == manager] # Will only be deployed on a manager node. networks: agent_network: driver: overlay attachable: true Setup data locations # One issue you gonna have with swarm is that it's not creating folder for declared volume on it's own so you will have to create it by yourself. # mkdir -p /mnt/docker_data/administration/portainer_data If like me you are using the same base directory for docker data, you can run this on the compose file to automatically build the folder. grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. Deploy Portainer stack # Deploy the Portainer stack by running docker stack deploy -c <path -to-docker-compose.yml> portainer Log into your new instance at any nodes on port 9000. You'll be prompted to set your admin user/password on first login. Start at \"Home\", and click on \"Primary\" to manage your swarm (you can manage multiple swarms via one Portainer instance using the agent Todo # Add image Add Some basic usage","title":"Portainer"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#portainer","text":"Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use. It consists of a single container that can run on any Docker engine (can be deployed as Linux container or a Windows native container, supports other platforms too). Portainer allows you to manage all your Docker resources (containers, images, volumes, networks and more!) It is compatible with the standalone Docker engine and with Docker Swarm mode.","title":"Portainer"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#preparaton","text":"","title":"Preparaton"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#setup-docker-swarm","text":"Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/administration # curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml","title":"Setup Docker Swarm"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#customisation","text":"By default portainer stack is using a normal docker volume. So if the node where the container will run fail, then we lost the data. To counter this we are going to use a local file shared accross nodes by GlusterFS. Edit portainer-agent-stack.yml Change: portainer_data:/data for /mnt/docker_data/administration/portainer_data:/data Remove: volumes: portainer_data: The file should look like this (I added some notes as comment): version: '3.2' services: agent: image: portainer/agent volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes networks: - agent_network deploy: mode: global # Will be deploy on all nodes manager or worker accross the swarm. placement: constraints: [node.platform.os == linux] # Will only run on Linux OS. portainer: image: portainer/portainer-ce command: -H tcp://tasks.agent:9001 --tlsskipverify ports: - \"9000:9000\" - \"8000:8000\" volumes: - /mnt/docker_data/administration/portainer_data:/data networks: - agent_network deploy: mode: replicated replicas: 1 # Only one container of this type gonna run at a time. placement: constraints: [node.role == manager] # Will only be deployed on a manager node. networks: agent_network: driver: overlay attachable: true","title":"Customisation"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#setup-data-locations","text":"One issue you gonna have with swarm is that it's not creating folder for declared volume on it's own so you will have to create it by yourself. # mkdir -p /mnt/docker_data/administration/portainer_data If like me you are using the same base directory for docker data, you can run this on the compose file to automatically build the folder. grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake.","title":"Setup data locations"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#deploy-portainer-stack","text":"Deploy the Portainer stack by running docker stack deploy -c <path -to-docker-compose.yml> portainer Log into your new instance at any nodes on port 9000. You'll be prompted to set your admin user/password on first login. Start at \"Home\", and click on \"Primary\" to manage your swarm (you can manage multiple swarms via one Portainer instance using the agent","title":"Deploy Portainer stack"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#todo","text":"Add image Add Some basic usage","title":"Todo"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/","text":"Shuffle # Shuffle is an Open Source SOAR I really appreciate as it does focus on CyberSecurity. To know more about it go there: https://medium.com/shuffle-automation Or there : Official Website Hi Frikky !!! Hoipefully, you will appreciate what you are seing here ^^ Preparaton # Setup data locations # Setup Docker Swarm # Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/SOAR Customisation # At the time of this writing I may be the only one shufflying around with swarm :p So there is not yet a stack ready compose file. Let's create one /mnt/configuration_data/compose_files/SOAR/shuffle.stack.yml version: '3.3' services: backend: image: ghcr.io/frikky/shuffle-backend:0.8.74 environment: DATASTORE_EMULATOR_HOST: shuffle-database:8000 HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_APP_DOWNLOAD_LOCATION: https://github.com/frikky/shuffle-apps SHUFFLE_APP_FORCE_UPDATE: 'false' SHUFFLE_APP_HOTLOAD_FOLDER: /shuffle-apps SHUFFLE_DEFAULT_APIKEY: zaCELgL.0imfnc8mVLWwsAawjYr4Rx-Af50DDqtlx SHUFFLE_DEFAULT_PASSWORD: Fr1kky1sN0t@D0g SHUFFLE_DEFAULT_USERNAME: admin SHUFFLE_DOWNLOAD_AUTH_BRANCH: '' SHUFFLE_FILE_LOCATION: /shuffle-files ports: - 5001:5001 volumes: - /var/run/docker.sock:/var/run/docker.sock - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-apps:/shuffle-apps - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-files:/shuffle-files networks: - shuffle logging: driver: json-file deploy: replicas: 3 database: image: frikky/shuffle:database ports: - 30002:8000 volumes: - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-database:/etc/shuffle networks: - shuffle logging: driver: json-file deploy: replicas: 1 frontend: image: ghcr.io/frikky/shuffle-frontend:0.8.74 environment: BACKEND_HOSTNAME: shuffle-backend ports: - 3001:80 - 3443:443 networks: - shuffle logging: driver: json-file deploy: replicas: 3 orborus: image: ghcr.io/frikky/shuffle-orborus:0.8.73 environment: BASE_URL: http://shuffle-backend:5001 CLEANUP: 'false' DOCKER_API_VERSION: '1.40' ENVIRONMENT_NAME: Shuffle HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_APP_SDK_VERSION: 0.8.77 SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: -0.8.60 SHUFFLE_ORBORUS_EXECUTION_TIMEOUT: '600' SHUFFLE_PASS_WORKER_PROXY: 'TRUE' SHUFFLE_WORKER_VERSION: 0.8.73 volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - shuffle logging: driver: json-file deploy: replicas: 3 networks: shuffle: driver: overlay Setup data locations # This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. # grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/SOAR/shuffle-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' Deploy Shuffle stack # Deploy the Shuffle stack by running docker stack deploy -c <path -to-docker-compose.yml> shuffle Log into your new instance at any nodes on port https://node:3443. Note # Have you may have seen, all except database have a replica set to 3. This is done so we don't create a messy mess on DB if writing are intented from different shuffle-database container at the same time on the same value which will lead to databse corruption. Anyway, this container is up in 7 second in case of failure of the node ^^ . App gonna be slow to run on it's first usage on all new nodes where Orborus is going to use the base image for the first time. As swarm distribute request around all replicas, sometime request gonna be fast as the node already run the image and sometime not. This all Replica and Clustering of Shuffle is not yet production ready as it is not yet properly implemented. I recommend using only one database and one Orborus for now.","title":"Shuffle"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#shuffle","text":"Shuffle is an Open Source SOAR I really appreciate as it does focus on CyberSecurity. To know more about it go there: https://medium.com/shuffle-automation Or there : Official Website Hi Frikky !!! Hoipefully, you will appreciate what you are seing here ^^","title":"Shuffle"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#preparaton","text":"","title":"Preparaton"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#setup-data-locations","text":"","title":"Setup data locations"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#setup-docker-swarm","text":"Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/SOAR","title":"Setup Docker Swarm"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#customisation","text":"At the time of this writing I may be the only one shufflying around with swarm :p So there is not yet a stack ready compose file. Let's create one /mnt/configuration_data/compose_files/SOAR/shuffle.stack.yml version: '3.3' services: backend: image: ghcr.io/frikky/shuffle-backend:0.8.74 environment: DATASTORE_EMULATOR_HOST: shuffle-database:8000 HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_APP_DOWNLOAD_LOCATION: https://github.com/frikky/shuffle-apps SHUFFLE_APP_FORCE_UPDATE: 'false' SHUFFLE_APP_HOTLOAD_FOLDER: /shuffle-apps SHUFFLE_DEFAULT_APIKEY: zaCELgL.0imfnc8mVLWwsAawjYr4Rx-Af50DDqtlx SHUFFLE_DEFAULT_PASSWORD: Fr1kky1sN0t@D0g SHUFFLE_DEFAULT_USERNAME: admin SHUFFLE_DOWNLOAD_AUTH_BRANCH: '' SHUFFLE_FILE_LOCATION: /shuffle-files ports: - 5001:5001 volumes: - /var/run/docker.sock:/var/run/docker.sock - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-apps:/shuffle-apps - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-files:/shuffle-files networks: - shuffle logging: driver: json-file deploy: replicas: 3 database: image: frikky/shuffle:database ports: - 30002:8000 volumes: - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-database:/etc/shuffle networks: - shuffle logging: driver: json-file deploy: replicas: 1 frontend: image: ghcr.io/frikky/shuffle-frontend:0.8.74 environment: BACKEND_HOSTNAME: shuffle-backend ports: - 3001:80 - 3443:443 networks: - shuffle logging: driver: json-file deploy: replicas: 3 orborus: image: ghcr.io/frikky/shuffle-orborus:0.8.73 environment: BASE_URL: http://shuffle-backend:5001 CLEANUP: 'false' DOCKER_API_VERSION: '1.40' ENVIRONMENT_NAME: Shuffle HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_APP_SDK_VERSION: 0.8.77 SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: -0.8.60 SHUFFLE_ORBORUS_EXECUTION_TIMEOUT: '600' SHUFFLE_PASS_WORKER_PROXY: 'TRUE' SHUFFLE_WORKER_VERSION: 0.8.73 volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - shuffle logging: driver: json-file deploy: replicas: 3 networks: shuffle: driver: overlay","title":"Customisation"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#setup-data-locations_1","text":"This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. # grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/SOAR/shuffle-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }'","title":"Setup data locations"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#deploy-shuffle-stack","text":"Deploy the Shuffle stack by running docker stack deploy -c <path -to-docker-compose.yml> shuffle Log into your new instance at any nodes on port https://node:3443.","title":"Deploy Shuffle stack"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#note","text":"Have you may have seen, all except database have a replica set to 3. This is done so we don't create a messy mess on DB if writing are intented from different shuffle-database container at the same time on the same value which will lead to databse corruption. Anyway, this container is up in 7 second in case of failure of the node ^^ . App gonna be slow to run on it's first usage on all new nodes where Orborus is going to use the base image for the first time. As swarm distribute request around all replicas, sometime request gonna be fast as the node already run the image and sometime not. This all Replica and Clustering of Shuffle is not yet production ready as it is not yet properly implemented. I recommend using only one database and one Orborus for now.","title":"Note"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/","text":"Installing GlusterFS Server # Adding Gluster Repository # # dnf install centos-release-gluster` Installing GlusterFS Server # # dnf install glusterfs-server A bit of simplification for management. # Generate ssh-key # # ssh-keygen -t ed25519 Push ssh-key to all nodes # # ssh-copy-id root@swarm2 # ssh-copy-id root@swarm3 This should be done on all nodes. Stop firewall # I highly doesn't recommend to stop the firewall but I don't want also that you will get frustrated because of it. So go look how to stop firewalld on CentOS using your favorite search engine. Configure GlusterFS # generate a private key for each node # # openssl genrsa -out /etc/ssl/glusterfs.key 4096` generate a signed certificate for each node # # openssl req -new -x509 -key /etc/ssl/glusterfs.key -subj \"/CN=`hostname -f`\" -out /etc/ssl/glusterfs.pem hostname -f : for full fqdn (swarm1.lab.local) hostname -s : for short version (swarm1) This is important to understand what are you going to use on all the nodes as it's going to be used to identify the allowed host to mount Gluster's volume. Enable and start service # # systemctl enable glusterd --now Peer nodes # From swarm1 run : # gluster peer probe swarm2 # gluster peer probe swarm3 Verify peers: # gluster peer status Number of Peers: 2 Hostname: swarm2 Uuid: 1085c1ff-6b79-4021-b6d4-93db50b8709a State: Peer in Cluster (Connected) Hostname: swarm3 Uuid: 68525dfd-e91c-4563-9e16-76131025dc68 State: Peer in Cluster (Connected) Create GlusterFS Volumes # We are going to create two separate volumes. One for the different compose files and on for the env volumes. We could have made only one GlusterFS volume for both but I like to separate elements. One thing you have to keep in mind, GlusterFS use bricks in between nodes that are folder. Do not write directly in it but on a mount of it. Compose and env Volumes # We gonna start by creating the brick # mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 # ssh root@swarm2 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 # ssh root@swarm3 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 Create a volume from it. # gluster volume create vol_configuration replica 3 transport tcp swarm{1..3}:/var/glusterfs/no-direct-write-here/vol_configuration/brick1 force volume create: vol_configuration: success: please start the volume to access data Let's activate tcp encryption. # gluster volume set vol_configuration client.ssl on volume set: success # gluster volume set vol_configuration server.ssl on volume set: success Time to allow our nodes to mount the volume as client and brick data as server. # gluster volume set vol_configuration auth.ssl-allow swarm1.lab.local,swarm2.lab.local,swarm3.lab.local volume set: success And now we start it. # gluster volume start vol_configuration volume start: vol_configuration: success Now we need to create a mount point for it. # mkdir /mnt/configuration_data/ # ssh root@swarm2 mkdir /mnt/configuration_data/ # ssh root@swarm3 mkdir /mnt/configuration_data/ And now we mount it. # mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data # ssh root@swarm2 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data # ssh root@swarm3 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data We check this is working as it should: # touch /mnt/configuration_data/youpi # ssh root@swarm2 ls /mnt/configuration_data # ssh root@swarm3 ls /mnt/configuration_data youpi And that ssl is activated grep ssl /var/log/glusterfs/mnt-configuration_data.log We want to have it mounted on startup so we add this to /etc/fstab of each node: echo \"localhost:/vol_configuration /mnt/configuration_data glusterfs defaults 0 0\" >> /etc/fstab Docker Data Volumes # Same as above but with the brick made here : /var/glusterfs/no-direct-write-here/vol_docker/brick1 And mountpoint: /mnt/docker_data/ Troubleshoot no GlusterFS mount on startup # I encoutered issue for the glusterFS volume to be mounted as startup as when the fstab is read by the system, glusterd is not yet running. The solution is to translate it in systemd-mount unit. # mkdir /etc/systemd/system/glusterfs.mount.d/ # ssh root@swarm2 mkdir /etc/systemd/system/configuration_data.mount.d/ # ssh root@swarm3 mkdir /etc/systemd/system/configuration_data.mount.d/ Official Documentation : https://docs.gluster.org/en/latest/","title":"Installing GlusterFS Server"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#installing-glusterfs-server","text":"","title":"Installing GlusterFS Server"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#adding-gluster-repository","text":"# dnf install centos-release-gluster`","title":"Adding Gluster Repository"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#installing-glusterfs-server_1","text":"# dnf install glusterfs-server","title":"Installing GlusterFS Server"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#a-bit-of-simplification-for-management","text":"","title":"A bit of simplification for management."},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#generate-ssh-key","text":"# ssh-keygen -t ed25519","title":"Generate ssh-key"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#push-ssh-key-to-all-nodes","text":"# ssh-copy-id root@swarm2 # ssh-copy-id root@swarm3 This should be done on all nodes.","title":"Push ssh-key to all nodes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#stop-firewall","text":"I highly doesn't recommend to stop the firewall but I don't want also that you will get frustrated because of it. So go look how to stop firewalld on CentOS using your favorite search engine.","title":"Stop firewall"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#configure-glusterfs","text":"","title":"Configure GlusterFS"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#generate-a-private-key-for-each-node","text":"# openssl genrsa -out /etc/ssl/glusterfs.key 4096`","title":"generate a private key for each node"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#generate-a-signed-certificate-for-each-node","text":"# openssl req -new -x509 -key /etc/ssl/glusterfs.key -subj \"/CN=`hostname -f`\" -out /etc/ssl/glusterfs.pem hostname -f : for full fqdn (swarm1.lab.local) hostname -s : for short version (swarm1) This is important to understand what are you going to use on all the nodes as it's going to be used to identify the allowed host to mount Gluster's volume.","title":"generate a signed certificate for each node"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#enable-and-start-service","text":"# systemctl enable glusterd --now","title":"Enable and start service"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#peer-nodes","text":"From swarm1 run : # gluster peer probe swarm2 # gluster peer probe swarm3 Verify peers: # gluster peer status Number of Peers: 2 Hostname: swarm2 Uuid: 1085c1ff-6b79-4021-b6d4-93db50b8709a State: Peer in Cluster (Connected) Hostname: swarm3 Uuid: 68525dfd-e91c-4563-9e16-76131025dc68 State: Peer in Cluster (Connected)","title":"Peer nodes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#create-glusterfs-volumes","text":"We are going to create two separate volumes. One for the different compose files and on for the env volumes. We could have made only one GlusterFS volume for both but I like to separate elements. One thing you have to keep in mind, GlusterFS use bricks in between nodes that are folder. Do not write directly in it but on a mount of it.","title":"Create GlusterFS Volumes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#compose-and-env-volumes","text":"We gonna start by creating the brick # mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 # ssh root@swarm2 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 # ssh root@swarm3 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 Create a volume from it. # gluster volume create vol_configuration replica 3 transport tcp swarm{1..3}:/var/glusterfs/no-direct-write-here/vol_configuration/brick1 force volume create: vol_configuration: success: please start the volume to access data Let's activate tcp encryption. # gluster volume set vol_configuration client.ssl on volume set: success # gluster volume set vol_configuration server.ssl on volume set: success Time to allow our nodes to mount the volume as client and brick data as server. # gluster volume set vol_configuration auth.ssl-allow swarm1.lab.local,swarm2.lab.local,swarm3.lab.local volume set: success And now we start it. # gluster volume start vol_configuration volume start: vol_configuration: success Now we need to create a mount point for it. # mkdir /mnt/configuration_data/ # ssh root@swarm2 mkdir /mnt/configuration_data/ # ssh root@swarm3 mkdir /mnt/configuration_data/ And now we mount it. # mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data # ssh root@swarm2 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data # ssh root@swarm3 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data We check this is working as it should: # touch /mnt/configuration_data/youpi # ssh root@swarm2 ls /mnt/configuration_data # ssh root@swarm3 ls /mnt/configuration_data youpi And that ssl is activated grep ssl /var/log/glusterfs/mnt-configuration_data.log We want to have it mounted on startup so we add this to /etc/fstab of each node: echo \"localhost:/vol_configuration /mnt/configuration_data glusterfs defaults 0 0\" >> /etc/fstab","title":"Compose and env Volumes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#docker-data-volumes","text":"Same as above but with the brick made here : /var/glusterfs/no-direct-write-here/vol_docker/brick1 And mountpoint: /mnt/docker_data/","title":"Docker Data Volumes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#troubleshoot-no-glusterfs-mount-on-startup","text":"I encoutered issue for the glusterFS volume to be mounted as startup as when the fstab is read by the system, glusterd is not yet running. The solution is to translate it in systemd-mount unit. # mkdir /etc/systemd/system/glusterfs.mount.d/ # ssh root@swarm2 mkdir /etc/systemd/system/configuration_data.mount.d/ # ssh root@swarm3 mkdir /etc/systemd/system/configuration_data.mount.d/ Official Documentation : https://docs.gluster.org/en/latest/","title":"Troubleshoot no GlusterFS mount on startup"}]}