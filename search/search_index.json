{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"!!! Warning \"WEBSITE UNDER CONSTRUCTION\" !!! Presentation # Hello, my name is of no importance as I am just a somebody and I don't dream to became an \"influencer\". I love Open Source and Security, and who I am today is mostly built on the sharing of others in the past and today, plus a little bit of stubbornness when things don't work. Hopefully what I share is going to help other to build tools and more importantly learn in the process of reading my misspelled words. Incoming posts # Thehive Clustering Cortex Clustering Misp and OpenCTI integration Reverse proxy and load balancing Use Keycloak to help with all the account and access. TODO # Create content Create moaarr content","title":"Home"},{"location":"#presentation","text":"Hello, my name is of no importance as I am just a somebody and I don't dream to became an \"influencer\". I love Open Source and Security, and who I am today is mostly built on the sharing of others in the past and today, plus a little bit of stubbornness when things don't work. Hopefully what I share is going to help other to build tools and more importantly learn in the process of reading my misspelled words.","title":"Presentation"},{"location":"#incoming-posts","text":"Thehive Clustering Cortex Clustering Misp and OpenCTI integration Reverse proxy and load balancing Use Keycloak to help with all the account and access.","title":"Incoming posts"},{"location":"#todo","text":"Create content Create moaarr content","title":"TODO"},{"location":"3-nodes-swarm/","text":"Docker the Zerg way # For a resilient installation we are going to build a docker swarm of 3 master nodes with also a worker role. (In swarm node can be master or worker - the one running containers - or both) It's important to understand the split brain concept. In the following cluster a quorum have to be kept to assure things don't go sideways. For the quorum to be achieve you should have 2 master nodes up. This is true for Swarm and Ceph. Nodes gonna be called: swarm1, swarm2, swarm3 +----------------------+ | +----------------------+ | [Node #1] |10.0.0.51 | | | [Node #1] |10.0.0.52 | | swarm1.lab.local +----------+----------+ swarm2.lab.local | | | | | | +----------------------+ | +----------------------+ | | | +----------------------+ | | [Node #3] |10.0.0.53 | | | swarm3.lab.local +----------+ | | +----------------------+ Why Swarm ? Swarm is an orchestration tool directly provided into docker from the version 1.12. It is easy to use compare to Kubernetes and easier to maintain for a small team. Highly-available (can tolerate the failure of a single component) Scalable (can add resource or capacity as required) Portable (run it on your home today, run it in everywhere tomorrow) Automated (requires minimal care and feeding) Why Ceph ? While Docker Swarm is great for keeping containers running and providing scaling capabilities, it does lack direct integration of persistent storage accross nodes. This means if you actually want your containers to keep any data persistent across restarts of services, you need to provide a shared storage to every docker nodes. This also means you shouldn't use docker volume declaration in you docker files.","title":"Architecture"},{"location":"3-nodes-swarm/#docker-the-zerg-way","text":"For a resilient installation we are going to build a docker swarm of 3 master nodes with also a worker role. (In swarm node can be master or worker - the one running containers - or both) It's important to understand the split brain concept. In the following cluster a quorum have to be kept to assure things don't go sideways. For the quorum to be achieve you should have 2 master nodes up. This is true for Swarm and Ceph. Nodes gonna be called: swarm1, swarm2, swarm3 +----------------------+ | +----------------------+ | [Node #1] |10.0.0.51 | | | [Node #1] |10.0.0.52 | | swarm1.lab.local +----------+----------+ swarm2.lab.local | | | | | | +----------------------+ | +----------------------+ | | | +----------------------+ | | [Node #3] |10.0.0.53 | | | swarm3.lab.local +----------+ | | +----------------------+ Why Swarm ? Swarm is an orchestration tool directly provided into docker from the version 1.12. It is easy to use compare to Kubernetes and easier to maintain for a small team. Highly-available (can tolerate the failure of a single component) Scalable (can add resource or capacity as required) Portable (run it on your home today, run it in everywhere tomorrow) Automated (requires minimal care and feeding) Why Ceph ? While Docker Swarm is great for keeping containers running and providing scaling capabilities, it does lack direct integration of persistent storage accross nodes. This means if you actually want your containers to keep any data persistent across restarts of services, you need to provide a shared storage to every docker nodes. This also means you shouldn't use docker volume declaration in you docker files.","title":"Docker the Zerg way"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/","text":"Building the Zerg Nest # Installation is based on a fresh Centos Stream minimal server. Therefore, it is only adapted for CentOS installation, it may or may not work on other distribution. Prerequise # 3 x nodes (bare-metal or VMs), each with: - A mainstream Linux OS (tested on either CentOS 8 Stream) - At least 2GB RAM (You will need more Powaaaa, for the stack I will present futher allongs) - At least 50GB disk space (but it'll be tight) - Connectivity to each other within the same subnet, and on a low-latency link (i.e., no WAN links) Installing Docker and Docker compose # Remove runc # dnf remove runc Adding docker repo # dnf install -y yum-utils dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo dnf install docker-ce docker-ce-cli containerd.io Installing docker-compose # dnf install python3-pip pip3 install --upgrade pip pip3 install setuptools-rust pip3 install docker-compose Let's start the whale # systemctl enable docker --now And now create a zerg warm from it # From swarm1: # docker swarm init Swarm initialized: current node ( ksdjlqsldjqsd2516685485 ) is now a manager. To add a worker to this swarm, run the following command: docker swarm \\ join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-814yer1z55vmyk2mwdhvjbob1 \\ 10 .0.0.51:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. We are going to add the two other nodes as manager: docker swarm join-token manager To add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-2k0vay9aub5eheikw7qi9v82o 10 .0.0.51:2377 Run the command provided on your other nodes to join them to the swarm as managers. After addition of a node, the output of docker node ls (on either host) should reflect all the nodes: # docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION p424u0yvmu0vvc8nsnspv83zw * swarm1.lab.local Ready Active Leader 20 .10.6 kg6w6ucpb2jf8v8xqai23pv3a swarm2.lab.local Ready Active Reachable 20 .10.6 lam7mgs5wus40iaydvp8u3ss7 swarm3.lab.local Ready Active Reachable 20 .10.6 You are now ready to swarm. Official Documentation : https://docs.docker.com/engine/swarm/ Little Network tweak # When running Docker Swarm on RedHat or CentOS VMs under VMware you may run into issues with communication over the swarm node routing mesh. This issue is traced back to UDP packets being dropped by the source node. Disabling checksum offloading appears to resolve this issue. Run the following on your VMs: ethtool -K [ interface ] tx-checksum-ip-generic off cat > /etc/NetworkManager/dispatcher.d/pre-up.d/10-tx-checksum-ip-generic <<'EOF' ethtool -K ens192 tx-checksum-ip-generic off EOF chmod +x /etc/NetworkManager/dispatcher.d/pre-up.d/10-tx-checksum-ip-generic Note: [interface] is your network adaptater so change it accordingly. Firewalling # Base configuration # Activate firewalld, you may want to check in /etc/firewalld/zones/ to check what is going to happen ^^ . One issue happening quite often is when you changed the default ssh port. As the ssh service declared in /usr/lib/firewalld/services/ssh.xml is referencing to port 22. If it's the case, copy the service.xml into /etc/firewalld/service and change the port of it. (And yes, this happen to me a few times) systemctl enable firewalld --now Unmask the service if needed : systemctl unmask firewalld By default, firewalld is having a public zone created. This public zone allow the use of ssh, cockpit, dhcpv6-client. cat /etc/firewalld/zones/public.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <zone> <short> Public </short> <description> For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted. </description> <service name= \"ssh\" /> <service name= \"dhcpv6-client\" /> <service name= \"cockpit\" /> </zone> If you don't use cockpit or dhcpv6-client, you can remove them from the configuration. For example to delete cockpit service: firewall-cmd --permamnent --zone = public --remove-service = cockpit firewall-cmd --reload Note here the parameters: --permanent: means the rules gonna last after service restart --zone: is used to indicate what zone should be modified, by default it's the public one --remove-service : remove the service declared in /etc/firewalld/services/ without the .xml ending firewall-cmd --reload is going to reload firewalld with latest configuration. Let's add some services firewall-cmd --permanent --zone = public --add-service = http firewall-cmd --permanent --zone = public --add-service = https firewall-cmd --reload We are going now to create a new zone representing the nodes of our cluster, and add sources to it (understand incoming traffic). firewall-cmd --permanent --new-zone = swarm firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.51 firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.52 firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.53 firewall-cmd --reload Let's check if sources where added firewall-cmd --zone = swarm --list-sources Ajouter les services n\u00e9cessaires au cluster : cp -a /usr/lib/firewalld/services/docker-swarm.xml /etc/firewalld/services/ firewall-cmd --zone = swarm --add-service = docker-swarm --permanent firewall-cmd --reload Docker Swarm # Let's add the service to the swarm zone: By default firewalld come bundled with some services. You can find them in /usr/lib/firewalld/services . I like to copy them in /etc/firewalld/services when I use them as it prevent it to be changed after an update. Firewalld prioritize service in \"/etc/firewalld/services/\" then in /usr/lib/firewalld/services . So let's copy the docker-swarm service: cp /usr/lib/firewalld/services/docker-swarm.xml /etc/firewalld/services/ cat /etc/firewalld/services/docker-swarm.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <service> <short> Docker integrated swarm mode </short> <description> Natively managed cluster of Docker Engines (>=1.12.0), where you deploy services. </description> <port port= \"2377\" protocol= \"tcp\" /> <port port= \"7946\" protocol= \"tcp\" /> <port port= \"7946\" protocol= \"udp\" /> <port port= \"4789\" protocol= \"udp\" /> <protocol value= \"esp\" /> </service> Then add it to the zone: firewall-cmd --permanent --zone = swarm --add-service = docker-swarm firewall-cmd --reload firewall-cmd --zone = swarm --list-services docker-swarm Now we did allow the port and protocols for the nodes to be used our cluster, and off course all those action have to be done on each host of the cluster.","title":"Building the Zerg Nest"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#building-the-zerg-nest","text":"Installation is based on a fresh Centos Stream minimal server. Therefore, it is only adapted for CentOS installation, it may or may not work on other distribution.","title":"Building the Zerg Nest"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#prerequise","text":"3 x nodes (bare-metal or VMs), each with: - A mainstream Linux OS (tested on either CentOS 8 Stream) - At least 2GB RAM (You will need more Powaaaa, for the stack I will present futher allongs) - At least 50GB disk space (but it'll be tight) - Connectivity to each other within the same subnet, and on a low-latency link (i.e., no WAN links)","title":"Prerequise"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#installing-docker-and-docker-compose","text":"","title":"Installing Docker and Docker compose"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#remove-runc","text":"dnf remove runc","title":"Remove runc"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#adding-docker-repo","text":"dnf install -y yum-utils dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo dnf install docker-ce docker-ce-cli containerd.io","title":"Adding docker repo"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#installing-docker-compose","text":"dnf install python3-pip pip3 install --upgrade pip pip3 install setuptools-rust pip3 install docker-compose","title":"Installing docker-compose"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#lets-start-the-whale","text":"systemctl enable docker --now","title":"Let's start the whale"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#and-now-create-a-zerg-warm-from-it","text":"From swarm1: # docker swarm init Swarm initialized: current node ( ksdjlqsldjqsd2516685485 ) is now a manager. To add a worker to this swarm, run the following command: docker swarm \\ join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-814yer1z55vmyk2mwdhvjbob1 \\ 10 .0.0.51:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. We are going to add the two other nodes as manager: docker swarm join-token manager To add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-2k0vay9aub5eheikw7qi9v82o 10 .0.0.51:2377 Run the command provided on your other nodes to join them to the swarm as managers. After addition of a node, the output of docker node ls (on either host) should reflect all the nodes: # docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION p424u0yvmu0vvc8nsnspv83zw * swarm1.lab.local Ready Active Leader 20 .10.6 kg6w6ucpb2jf8v8xqai23pv3a swarm2.lab.local Ready Active Reachable 20 .10.6 lam7mgs5wus40iaydvp8u3ss7 swarm3.lab.local Ready Active Reachable 20 .10.6 You are now ready to swarm. Official Documentation : https://docs.docker.com/engine/swarm/","title":"And now create a zerg warm from it"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#little-network-tweak","text":"When running Docker Swarm on RedHat or CentOS VMs under VMware you may run into issues with communication over the swarm node routing mesh. This issue is traced back to UDP packets being dropped by the source node. Disabling checksum offloading appears to resolve this issue. Run the following on your VMs: ethtool -K [ interface ] tx-checksum-ip-generic off cat > /etc/NetworkManager/dispatcher.d/pre-up.d/10-tx-checksum-ip-generic <<'EOF' ethtool -K ens192 tx-checksum-ip-generic off EOF chmod +x /etc/NetworkManager/dispatcher.d/pre-up.d/10-tx-checksum-ip-generic Note: [interface] is your network adaptater so change it accordingly.","title":"Little Network tweak"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#firewalling","text":"","title":"Firewalling"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#base-configuration","text":"Activate firewalld, you may want to check in /etc/firewalld/zones/ to check what is going to happen ^^ . One issue happening quite often is when you changed the default ssh port. As the ssh service declared in /usr/lib/firewalld/services/ssh.xml is referencing to port 22. If it's the case, copy the service.xml into /etc/firewalld/service and change the port of it. (And yes, this happen to me a few times) systemctl enable firewalld --now Unmask the service if needed : systemctl unmask firewalld By default, firewalld is having a public zone created. This public zone allow the use of ssh, cockpit, dhcpv6-client. cat /etc/firewalld/zones/public.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <zone> <short> Public </short> <description> For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted. </description> <service name= \"ssh\" /> <service name= \"dhcpv6-client\" /> <service name= \"cockpit\" /> </zone> If you don't use cockpit or dhcpv6-client, you can remove them from the configuration. For example to delete cockpit service: firewall-cmd --permamnent --zone = public --remove-service = cockpit firewall-cmd --reload Note here the parameters: --permanent: means the rules gonna last after service restart --zone: is used to indicate what zone should be modified, by default it's the public one --remove-service : remove the service declared in /etc/firewalld/services/ without the .xml ending firewall-cmd --reload is going to reload firewalld with latest configuration. Let's add some services firewall-cmd --permanent --zone = public --add-service = http firewall-cmd --permanent --zone = public --add-service = https firewall-cmd --reload We are going now to create a new zone representing the nodes of our cluster, and add sources to it (understand incoming traffic). firewall-cmd --permanent --new-zone = swarm firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.51 firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.52 firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.53 firewall-cmd --reload Let's check if sources where added firewall-cmd --zone = swarm --list-sources Ajouter les services n\u00e9cessaires au cluster : cp -a /usr/lib/firewalld/services/docker-swarm.xml /etc/firewalld/services/ firewall-cmd --zone = swarm --add-service = docker-swarm --permanent firewall-cmd --reload","title":"Base configuration"},{"location":"3-nodes-swarm/DockerSwarm/01%20Installation/#docker-swarm","text":"Let's add the service to the swarm zone: By default firewalld come bundled with some services. You can find them in /usr/lib/firewalld/services . I like to copy them in /etc/firewalld/services when I use them as it prevent it to be changed after an update. Firewalld prioritize service in \"/etc/firewalld/services/\" then in /usr/lib/firewalld/services . So let's copy the docker-swarm service: cp /usr/lib/firewalld/services/docker-swarm.xml /etc/firewalld/services/ cat /etc/firewalld/services/docker-swarm.xml <?xml version=\"1.0\" encoding=\"utf-8\"?> <service> <short> Docker integrated swarm mode </short> <description> Natively managed cluster of Docker Engines (>=1.12.0), where you deploy services. </description> <port port= \"2377\" protocol= \"tcp\" /> <port port= \"7946\" protocol= \"tcp\" /> <port port= \"7946\" protocol= \"udp\" /> <port port= \"4789\" protocol= \"udp\" /> <protocol value= \"esp\" /> </service> Then add it to the zone: firewall-cmd --permanent --zone = swarm --add-service = docker-swarm firewall-cmd --reload firewall-cmd --zone = swarm --list-services docker-swarm Now we did allow the port and protocols for the nodes to be used our cluster, and off course all those action have to be done on each host of the cluster.","title":"Docker Swarm"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/","text":"Data Persistance and Sharing with Ceph # While Docker Swarm is great for keeping containers running (and restarting those that fail), it does nothing for persistent storage. This means if you actually want your containers to keep any data persistent across restarts (hint: you do!), you need to provide shared storage to every docker node. Ceph is an open-source software (software-defined storage) storage platform, implements object storage on a single distributed computer cluster, and provides 3-in-1 interfaces for object-, block and file-level storage. Ceph aims primarily for completely distributed operation without a single point of failure, scalable to the exabyte level, and freely available. There are several ways to install Ceph. Choose the method that best suits your needs. For recommendation on ceph documentation is used cephadm. Cephadm installs and manages a Ceph cluster using containers and systemd, with tight integration with the CLI and dashboard GUI. Note on Cephadm: - Only supports Octopus and newer releases. - Fully integrated with the new orchestration API and fully supports the new CLI and dashboard features to manage cluster deployment. - Requires container support (podman or docker) and Python 3. Prerequise # 3 x Virtual Machines (configured earlier for lab or dedicated for production), each with: Support for \"modern\" versions of Python and LVM At least 2GB RAM At least 50GB disk space (but it'll be tight) Connectivity to each other within the same subnet, and on a low-latency link (i.e., no WAN links) At least an additionnal disk dedicated to the Ceph OSD (add it to previous host if needed) Each node should have the IP of every other participating node hard-coded in /etc/hosts (including its own IP) /etc/hosts # Add those record to /etc/hosts, edit IP if required. # Ceph Hosts 10 .0.0.41 ceph1 10 .0.0.42 ceph3 10 .0.0.43 ceph3 # Swarm Hosts 10 .0.0.51 swarm1 10 .0.0.52 swarm2 10 .0.0.53 swarm3 Choose your first manager node # One of your nodes will become the cephadm \"master\" node. Although all nodes will participate in the Ceph cluster, the master node will be the node which we bootstrap ceph on. It's also the node which will run the Ceph dashboard, and on which future upgrades will be processed. It doesn't matter which node you pick, and the cluster itself will operate in the event of a loss of the master node (although you won't see the dashboard) Install cephadm on master node # Run the following on the master node: RELEASE = \"quincy\" # Use curl to fetch the most recent version of the standalone script curl --silent --remote-name --location https://raw.githubusercontent.com/ceph/ceph/ $RELEASE /src/cephadm/cephadm #Make the cephadm script executable: chmod +x cephadm # To install the packages that provide the cephadm command, run the following commands: ./cephadm add-repo --release $RELEASE ./cephadm install #Install Ceph-common and Confirm that cephadm is now in your PATH by running which: dnf install -y Ceph-common which cephadm Bootstrap new Ceph cluster # The first step in creating a new Ceph cluster is running the cephadm bootstrap command on the Ceph cluster\u2019s first host. The act of running the cephadm bootstrap command on the Ceph cluster\u2019s first host creates the Ceph cluster\u2019s first \u201cmonitor daemon\u201d, and that monitor daemon needs an IP address. You must pass the IP address of the Ceph cluster\u2019s first host to the ceph bootstrap command, so you\u2019ll need to know the IP address of that host. MYIP = ` ip route get 1 .1.1.1 | grep -oP 'src \\K\\S+' ` mkdir -p /etc/ceph cephadm bootstrap --mon-ip $MYIP This command will: Create a monitor and manager daemon for the new cluster on the local host. Generate a new SSH key for the Ceph cluster and add it to the root user\u2019s /root/.ssh/authorized_keys file. Write a copy of the public key to /etc/ceph/ceph.pub . Write a minimal configuration file to /etc/ceph/ceph.conf . This file is needed to communicate with the new cluster. Write a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring . Add the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring. Check Ceph dashboard, access IP address of ceph01 https://192.168.1.231:8443/ and use credentials from the cephadm bootstrap output then set a new password Confirm that the ceph command is accessible with: ceph -v Result: ceph version 17 .2.1 ( ec95624474b1871a821a912b8c3af68f8f8e7aa1 ) quincy ( stable ) Check status of ceph cluster, OK for [HEALTH_WARN] because OSDs are not added yet ceph -s Result: cluster: id: 588df728-316c-11ec-b956-005056aea762 health: HEALTH_WARN OSD count 0 < osd_pool_default_size 3 services: mon: 1 daemons, quorum ceph01 ( age 14m ) mgr: ceph01.wgdjcn ( active, since 12m ) osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: Verify containers are running for each service and check status for systemd service for each containers docker ps | grep ceph systemctl status Ceph-* --no-pager ``` ssssss ## Adding hosts to the cluster. To add each new host to the cluster, perform two steps: ``` bash # Install the cluster\u2019s public SSH key in the new host\u2019s root user\u2019s ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph2 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph3 #Tell Ceph that the new node is part of the cluster, make sure python3 installed and available on new node ceph orch host add ceph2 ceph orch host add ceph3 #Check the added host ceph orch host ls Result: HOST ADDR LABELS STATUS ceph1 192 .168.1.231 _admin ceph2 192 .168.1.232 ceph3 192 .168.1.233 Deploy OSDs to the cluster # Run this command to display an inventory of storage devices on all cluster hosts: ceph orch device ls Result: Hostname Path Type Serial Size Health Ident Fault Available ceph1 /dev/sdb ssd 150G Unknown N/A N/A Yes ceph2 /dev/sdb ssd 150G Unknown N/A N/A Yes ceph3 /dev/sdb ssd 150G Unknown N/A N/A Yes Tell Ceph to consume any available and unused storage device execute ceph orch apply osd --all-available-devices ceph orch apply osd --all-available-devices ceph -s Result:ssss cluster: id: 588df728-316c-11ec-b956-005056aea762 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 ( age 5m ) mgr: ceph1.wgdjcn ( active, since 41m ) , standbys: ceph02.rmltzq osd: 9 osds: 0 up, 9 in ( since 10s ) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: ceph osd tree Result: ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0 .08817 root default -5 0 .02939 host ceph1 1 ssd 0 .00980 osd.0 up 1 .00000 1 .00000 -7 0 .02939 host ceph2 0 ssd 0 .00980 osd.1 up 1 .00000 1 .00000 -3 0 .02939 host ceph3 2 ssd 0 .00980 osd.3 up 1 .00000 1 .00000 Deploy Ceph-mon (ceph monitor daemon) # Ceph-mon is the cluster monitor daemon for the Ceph distributed file system. One or more instances of Ceph-mon form a Paxos part-time parliament cluster that provides extremely reliable and durable storage of cluster membership, configuration, and state. Add Ceph-mon to all node using placement option ceph orch apply mon --placement = \"ceph1,ceph2,ceph3\" ceph orch ps | grep mon Result: mon.ceph1 ceph1 running ( 63m ) 7m ago 63m 209M 2048M 16 .2.6 02a72919e474 952d7 mon.ceph2 ceph2 running ( 27m ) 7m ago 27m 104M 2048M 16 .2.6 02a72919e474 f2d22 mon.ceph3 ceph3 running ( 25m ) 7m ago 25m 104M 2048M 16 .2.6 02a72919e474 bcc00 Result: Deploy Ceph-mgr (ceph manager daemon) # The Ceph Manager daemon (Ceph-mgr) runs alongside monitor daemons, to provide additional monitoring and interfaces to external monitoring and management systems. ceph orch apply mgr --placement = \"ceph1,ceph2,ceph3\" ceph orch ps | grep mgr Result: mgr.ceph1.wgdjcn ceph1 *:9283 running ( 64m ) 8m ago 64m 465M - 16 .2.6 02a72919e474 c58a64249f9b mgr.ceph2.rmltzq ceph2 *:8443,9283 running ( 29m ) 8m ago 29m 385M - 16 .2.6 02a72919e474 36f7f6a02896 mgr.ceph3.lhwjwd ceph3 *:8443,9283 running ( 7s ) 2s ago 6s 205M - 16 .2.6 02a72919e474 c740f964b2de Set _admin label on all nodes # The orchestrator supports assigning labels to hosts. Labels are free form and have no particular meaning by itself and each host can have multiple labels. They can be used to specify placement of daemons. But the _admin force the replication of change on ceph.conf to all node with this tag. Official note: By default, a ceph.conf file and a copy of the client.admin keyring are maintained in /etc/ceph on all hosts with the _admin label, which is initially applied only to the bootstrap host. We usually recommend that one or more other hosts be given the _admin label so that the Ceph CLI (e.g., via cephadm shell) is easily accessible on multiple hosts. To add the _admin label to additional host(s) ceph orch host label add ceph3 _admin ceph orch host label add ceph3 _admin Prepare for cephFS mount # It's now necessary to tranfer the following files to your other nodes, so that cephadm can add them to your cluster, and so that they'll be able to mount the cephfs when we're done: Path on master Path on non-master /etc/ceph/ceph.conf /etc/ceph/ceph.conf /etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring Setup CephFS # On the master node, create a cephfs volume in your cluster, by running ceph fs volume create data . Ceph will handle the necessary orchestration itself, creating the necessary pool, mds daemon, etc. You can watch the progress by running ceph fs ls (to see the fs is configured), and ceph -s to wait for HEALTH_OK Reproduce the following on each node: mkdir /mnt/swarm echo -e \" # Mount cephfs volume \\n ceph1,ceph2,ceph3:/ /mnt/swarm ceph name=admin,noatime,_netdev 0 0\" >> /etc/fstab mount -a You can now play around and copy delete data on /mnt/swarm and check the replication accross the nodes. References # https://docs.ceph.com/en/latest/cephadm/install/","title":"Data Persistance and Sharing with Ceph"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#data-persistance-and-sharing-with-ceph","text":"While Docker Swarm is great for keeping containers running (and restarting those that fail), it does nothing for persistent storage. This means if you actually want your containers to keep any data persistent across restarts (hint: you do!), you need to provide shared storage to every docker node. Ceph is an open-source software (software-defined storage) storage platform, implements object storage on a single distributed computer cluster, and provides 3-in-1 interfaces for object-, block and file-level storage. Ceph aims primarily for completely distributed operation without a single point of failure, scalable to the exabyte level, and freely available. There are several ways to install Ceph. Choose the method that best suits your needs. For recommendation on ceph documentation is used cephadm. Cephadm installs and manages a Ceph cluster using containers and systemd, with tight integration with the CLI and dashboard GUI. Note on Cephadm: - Only supports Octopus and newer releases. - Fully integrated with the new orchestration API and fully supports the new CLI and dashboard features to manage cluster deployment. - Requires container support (podman or docker) and Python 3.","title":"Data Persistance and Sharing with Ceph"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#prerequise","text":"3 x Virtual Machines (configured earlier for lab or dedicated for production), each with: Support for \"modern\" versions of Python and LVM At least 2GB RAM At least 50GB disk space (but it'll be tight) Connectivity to each other within the same subnet, and on a low-latency link (i.e., no WAN links) At least an additionnal disk dedicated to the Ceph OSD (add it to previous host if needed) Each node should have the IP of every other participating node hard-coded in /etc/hosts (including its own IP)","title":"Prerequise"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#etchosts","text":"Add those record to /etc/hosts, edit IP if required. # Ceph Hosts 10 .0.0.41 ceph1 10 .0.0.42 ceph3 10 .0.0.43 ceph3 # Swarm Hosts 10 .0.0.51 swarm1 10 .0.0.52 swarm2 10 .0.0.53 swarm3","title":"/etc/hosts"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#choose-your-first-manager-node","text":"One of your nodes will become the cephadm \"master\" node. Although all nodes will participate in the Ceph cluster, the master node will be the node which we bootstrap ceph on. It's also the node which will run the Ceph dashboard, and on which future upgrades will be processed. It doesn't matter which node you pick, and the cluster itself will operate in the event of a loss of the master node (although you won't see the dashboard)","title":"Choose your first manager node"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#install-cephadm-on-master-node","text":"Run the following on the master node: RELEASE = \"quincy\" # Use curl to fetch the most recent version of the standalone script curl --silent --remote-name --location https://raw.githubusercontent.com/ceph/ceph/ $RELEASE /src/cephadm/cephadm #Make the cephadm script executable: chmod +x cephadm # To install the packages that provide the cephadm command, run the following commands: ./cephadm add-repo --release $RELEASE ./cephadm install #Install Ceph-common and Confirm that cephadm is now in your PATH by running which: dnf install -y Ceph-common which cephadm","title":"Install cephadm on master node"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#bootstrap-new-ceph-cluster","text":"The first step in creating a new Ceph cluster is running the cephadm bootstrap command on the Ceph cluster\u2019s first host. The act of running the cephadm bootstrap command on the Ceph cluster\u2019s first host creates the Ceph cluster\u2019s first \u201cmonitor daemon\u201d, and that monitor daemon needs an IP address. You must pass the IP address of the Ceph cluster\u2019s first host to the ceph bootstrap command, so you\u2019ll need to know the IP address of that host. MYIP = ` ip route get 1 .1.1.1 | grep -oP 'src \\K\\S+' ` mkdir -p /etc/ceph cephadm bootstrap --mon-ip $MYIP This command will: Create a monitor and manager daemon for the new cluster on the local host. Generate a new SSH key for the Ceph cluster and add it to the root user\u2019s /root/.ssh/authorized_keys file. Write a copy of the public key to /etc/ceph/ceph.pub . Write a minimal configuration file to /etc/ceph/ceph.conf . This file is needed to communicate with the new cluster. Write a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring . Add the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring. Check Ceph dashboard, access IP address of ceph01 https://192.168.1.231:8443/ and use credentials from the cephadm bootstrap output then set a new password Confirm that the ceph command is accessible with: ceph -v Result: ceph version 17 .2.1 ( ec95624474b1871a821a912b8c3af68f8f8e7aa1 ) quincy ( stable ) Check status of ceph cluster, OK for [HEALTH_WARN] because OSDs are not added yet ceph -s Result: cluster: id: 588df728-316c-11ec-b956-005056aea762 health: HEALTH_WARN OSD count 0 < osd_pool_default_size 3 services: mon: 1 daemons, quorum ceph01 ( age 14m ) mgr: ceph01.wgdjcn ( active, since 12m ) osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: Verify containers are running for each service and check status for systemd service for each containers docker ps | grep ceph systemctl status Ceph-* --no-pager ``` ssssss ## Adding hosts to the cluster. To add each new host to the cluster, perform two steps: ``` bash # Install the cluster\u2019s public SSH key in the new host\u2019s root user\u2019s ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph2 ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph3 #Tell Ceph that the new node is part of the cluster, make sure python3 installed and available on new node ceph orch host add ceph2 ceph orch host add ceph3 #Check the added host ceph orch host ls Result: HOST ADDR LABELS STATUS ceph1 192 .168.1.231 _admin ceph2 192 .168.1.232 ceph3 192 .168.1.233","title":"Bootstrap new Ceph cluster"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#deploy-osds-to-the-cluster","text":"Run this command to display an inventory of storage devices on all cluster hosts: ceph orch device ls Result: Hostname Path Type Serial Size Health Ident Fault Available ceph1 /dev/sdb ssd 150G Unknown N/A N/A Yes ceph2 /dev/sdb ssd 150G Unknown N/A N/A Yes ceph3 /dev/sdb ssd 150G Unknown N/A N/A Yes Tell Ceph to consume any available and unused storage device execute ceph orch apply osd --all-available-devices ceph orch apply osd --all-available-devices ceph -s Result:ssss cluster: id: 588df728-316c-11ec-b956-005056aea762 health: HEALTH_OK services: mon: 3 daemons, quorum ceph1,ceph2,ceph3 ( age 5m ) mgr: ceph1.wgdjcn ( active, since 41m ) , standbys: ceph02.rmltzq osd: 9 osds: 0 up, 9 in ( since 10s ) data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: ceph osd tree Result: ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0 .08817 root default -5 0 .02939 host ceph1 1 ssd 0 .00980 osd.0 up 1 .00000 1 .00000 -7 0 .02939 host ceph2 0 ssd 0 .00980 osd.1 up 1 .00000 1 .00000 -3 0 .02939 host ceph3 2 ssd 0 .00980 osd.3 up 1 .00000 1 .00000","title":"Deploy OSDs to the cluster"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#deploy-ceph-mon-ceph-monitor-daemon","text":"Ceph-mon is the cluster monitor daemon for the Ceph distributed file system. One or more instances of Ceph-mon form a Paxos part-time parliament cluster that provides extremely reliable and durable storage of cluster membership, configuration, and state. Add Ceph-mon to all node using placement option ceph orch apply mon --placement = \"ceph1,ceph2,ceph3\" ceph orch ps | grep mon Result: mon.ceph1 ceph1 running ( 63m ) 7m ago 63m 209M 2048M 16 .2.6 02a72919e474 952d7 mon.ceph2 ceph2 running ( 27m ) 7m ago 27m 104M 2048M 16 .2.6 02a72919e474 f2d22 mon.ceph3 ceph3 running ( 25m ) 7m ago 25m 104M 2048M 16 .2.6 02a72919e474 bcc00 Result:","title":"Deploy Ceph-mon (ceph monitor daemon)"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#deploy-ceph-mgr-ceph-manager-daemon","text":"The Ceph Manager daemon (Ceph-mgr) runs alongside monitor daemons, to provide additional monitoring and interfaces to external monitoring and management systems. ceph orch apply mgr --placement = \"ceph1,ceph2,ceph3\" ceph orch ps | grep mgr Result: mgr.ceph1.wgdjcn ceph1 *:9283 running ( 64m ) 8m ago 64m 465M - 16 .2.6 02a72919e474 c58a64249f9b mgr.ceph2.rmltzq ceph2 *:8443,9283 running ( 29m ) 8m ago 29m 385M - 16 .2.6 02a72919e474 36f7f6a02896 mgr.ceph3.lhwjwd ceph3 *:8443,9283 running ( 7s ) 2s ago 6s 205M - 16 .2.6 02a72919e474 c740f964b2de","title":"Deploy Ceph-mgr (ceph manager daemon)"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#set-_admin-label-on-all-nodes","text":"The orchestrator supports assigning labels to hosts. Labels are free form and have no particular meaning by itself and each host can have multiple labels. They can be used to specify placement of daemons. But the _admin force the replication of change on ceph.conf to all node with this tag. Official note: By default, a ceph.conf file and a copy of the client.admin keyring are maintained in /etc/ceph on all hosts with the _admin label, which is initially applied only to the bootstrap host. We usually recommend that one or more other hosts be given the _admin label so that the Ceph CLI (e.g., via cephadm shell) is easily accessible on multiple hosts. To add the _admin label to additional host(s) ceph orch host label add ceph3 _admin ceph orch host label add ceph3 _admin","title":"Set _admin label on all nodes"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#prepare-for-cephfs-mount","text":"It's now necessary to tranfer the following files to your other nodes, so that cephadm can add them to your cluster, and so that they'll be able to mount the cephfs when we're done: Path on master Path on non-master /etc/ceph/ceph.conf /etc/ceph/ceph.conf /etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring","title":"Prepare for cephFS mount"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#setup-cephfs","text":"On the master node, create a cephfs volume in your cluster, by running ceph fs volume create data . Ceph will handle the necessary orchestration itself, creating the necessary pool, mds daemon, etc. You can watch the progress by running ceph fs ls (to see the fs is configured), and ceph -s to wait for HEALTH_OK Reproduce the following on each node: mkdir /mnt/swarm echo -e \" # Mount cephfs volume \\n ceph1,ceph2,ceph3:/ /mnt/swarm ceph name=admin,noatime,_netdev 0 0\" >> /etc/fstab mount -a You can now play around and copy delete data on /mnt/swarm and check the replication accross the nodes.","title":"Setup CephFS"},{"location":"3-nodes-swarm/DockerSwarm/02%20Ceph/#references","text":"https://docs.ceph.com/en/latest/cephadm/install/","title":"References"},{"location":"3-nodes-swarm/DockerSwarm/03%20Basic%20Stacks/","text":"Basic Stacks # This is no basic stack like for starter, but basics as needed in any deployement of my special stacks recipes ! Traefik # Portainer #","title":"Basic Stacks"},{"location":"3-nodes-swarm/DockerSwarm/03%20Basic%20Stacks/#basic-stacks","text":"This is no basic stack like for starter, but basics as needed in any deployement of my special stacks recipes !","title":"Basic Stacks"},{"location":"3-nodes-swarm/DockerSwarm/03%20Basic%20Stacks/#traefik","text":"","title":"Traefik"},{"location":"3-nodes-swarm/DockerSwarm/03%20Basic%20Stacks/#portainer","text":"","title":"Portainer"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/","text":"Installing GlusterFS Server # Adding Gluster Repository # # dnf install centos-release-gluster` Installing GlusterFS Server # # dnf install glusterfs-server A bit of simplification for management. # Generate ssh-key # # ssh-keygen -t ed25519 Push ssh-key to all nodes # # ssh-copy-id root@swarm2 # ssh-copy-id root@swarm3 This should be done on all nodes. Stop firewall # For now we will stop the firewall so it's not going to interfere in this configuration. But I higly recommend to look on the documentation to know what port should be open. I will not give the command in here to do so, RTFM. Configure GlusterFS # generate a private key for each node # # openssl genrsa -out /etc/ssl/glusterfs.key 4096` generate a signed certificate for each node # # openssl req -new -x509 -key /etc/ssl/glusterfs.key -subj \"/CN=`hostname -f`\" -out /etc/ssl/glusterfs.pem hostname -f : for full fqdn (swarm1.lab.local) hostname -s : for short version (swarm1) This is important to understand what are you going to use on all the nodes as it's going to be used to identify the allowed host to mount Gluster's volume. Enable and start service # # systemctl enable glusterd --now Peer nodes # From swarm1 run : # gluster peer probe swarm2 # gluster peer probe swarm3 Verify peers: # gluster peer status Number of Peers: 2 Hostname: swarm2 Uuid: 1085c1ff-6b79-4021-b6d4-93db50b8709a State: Peer in Cluster (Connected) Hostname: swarm3 Uuid: 68525dfd-e91c-4563-9e16-76131025dc68 State: Peer in Cluster (Connected) Create GlusterFS Volumes # We are going to create two separated volumes. One for the different compose files and on for the env volumes. We could have made only one GlusterFS volume for both but I like to separate elements. One thing you have to keep in mind, GlusterFS use bricks in between nodes that are folder. Do not write directly in it but on a mount of it. Compose and env Volumes # We gonna start by creating the brick # mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 # ssh root@swarm2 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 # ssh root@swarm3 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 Create a volume from it. # gluster volume create vol_configuration replica 3 transport tcp swarm{1..3}:/var/glusterfs/no-direct-write-here/vol_configuration/brick1 force volume create: vol_configuration: success: please start the volume to access data Let's activate tcp encryption. # gluster volume set vol_configuration client.ssl on volume set: success # gluster volume set vol_configuration server.ssl on volume set: success Time to allow our nodes to mount the volume as client and brick data as server. # gluster volume set vol_configuration auth.ssl-allow swarm1.lab.local,swarm2.lab.local,swarm3.lab.local volume set: success And now we start it. # gluster volume start vol_configuration volume start: vol_configuration: success Now we need to create a mount point for it. # mkdir /mnt/configuration_data/ # ssh root@swarm2 mkdir /mnt/configuration_data/ # ssh root@swarm3 mkdir /mnt/configuration_data/ And now we mount it. # mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data # ssh root@swarm2 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data # ssh root@swarm3 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data We check this is working as it should: # touch /mnt/configuration_data/youpi # ssh root@swarm2 ls /mnt/configuration_data # ssh root@swarm3 ls /mnt/configuration_data youpi And that ssl is activated grep ssl /var/log/glusterfs/mnt-configuration_data.log We want to have it mounted on startup so we add this to /etc/fstab of each node: echo \"localhost:/vol_configuration /mnt/configuration_data glusterfs defaults,_netdev,noauto,x-systemd.automount,x-systemd.requires=gulsterd.service,x-systemd.after=gulsterd.service 0 0 All the x-systemd option are here to make sure the volume is mounted after glusterFS service is running. For more information read it here https://www.freedesktop.org/software/systemd/man/systemd.mount.html . Docker Data Volumes # Same as above but with the brick made here : /var/glusterfs/no-direct-write-here/vol_docker/brick1 And mountpoint: /mnt/docker_data/ Funny stuff # Imagine you got your brick (/var/glusterfs/no-direct-write-here/vol_configuration/brick1) and swarm1 on a dedicated hard drive and this hard drive died for some reason. You still could use the hosts to run containers, just mount the volume from an other node mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data You could have also use two bricks or more instead of one to create the GlusterFS volume quite like you will have done it with Raid1. But this is to be read on the Official documentation of the project. Troubleshoot no GlusterFS mount on startup # I encountered issue for the glusterFS saying a fresh peer couldn't be added because it had existing volumes. This was due to the fact I forgot to recreate the gluster.ca file to add it's public certificate. Official Documentation : https://docs.gluster.org/en/latest/","title":"Installing GlusterFS Server"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#installing-glusterfs-server","text":"","title":"Installing GlusterFS Server"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#adding-gluster-repository","text":"# dnf install centos-release-gluster`","title":"Adding Gluster Repository"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#installing-glusterfs-server_1","text":"# dnf install glusterfs-server","title":"Installing GlusterFS Server"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#a-bit-of-simplification-for-management","text":"","title":"A bit of simplification for management."},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#generate-ssh-key","text":"# ssh-keygen -t ed25519","title":"Generate ssh-key"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#push-ssh-key-to-all-nodes","text":"# ssh-copy-id root@swarm2 # ssh-copy-id root@swarm3 This should be done on all nodes.","title":"Push ssh-key to all nodes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#stop-firewall","text":"For now we will stop the firewall so it's not going to interfere in this configuration. But I higly recommend to look on the documentation to know what port should be open. I will not give the command in here to do so, RTFM.","title":"Stop firewall"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#configure-glusterfs","text":"","title":"Configure GlusterFS"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#generate-a-private-key-for-each-node","text":"# openssl genrsa -out /etc/ssl/glusterfs.key 4096`","title":"generate a private key for each node"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#generate-a-signed-certificate-for-each-node","text":"# openssl req -new -x509 -key /etc/ssl/glusterfs.key -subj \"/CN=`hostname -f`\" -out /etc/ssl/glusterfs.pem hostname -f : for full fqdn (swarm1.lab.local) hostname -s : for short version (swarm1) This is important to understand what are you going to use on all the nodes as it's going to be used to identify the allowed host to mount Gluster's volume.","title":"generate a signed certificate for each node"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#enable-and-start-service","text":"# systemctl enable glusterd --now","title":"Enable and start service"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#peer-nodes","text":"From swarm1 run : # gluster peer probe swarm2 # gluster peer probe swarm3 Verify peers: # gluster peer status Number of Peers: 2 Hostname: swarm2 Uuid: 1085c1ff-6b79-4021-b6d4-93db50b8709a State: Peer in Cluster (Connected) Hostname: swarm3 Uuid: 68525dfd-e91c-4563-9e16-76131025dc68 State: Peer in Cluster (Connected)","title":"Peer nodes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#create-glusterfs-volumes","text":"We are going to create two separated volumes. One for the different compose files and on for the env volumes. We could have made only one GlusterFS volume for both but I like to separate elements. One thing you have to keep in mind, GlusterFS use bricks in between nodes that are folder. Do not write directly in it but on a mount of it.","title":"Create GlusterFS Volumes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#compose-and-env-volumes","text":"We gonna start by creating the brick # mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 # ssh root@swarm2 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 # ssh root@swarm3 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 Create a volume from it. # gluster volume create vol_configuration replica 3 transport tcp swarm{1..3}:/var/glusterfs/no-direct-write-here/vol_configuration/brick1 force volume create: vol_configuration: success: please start the volume to access data Let's activate tcp encryption. # gluster volume set vol_configuration client.ssl on volume set: success # gluster volume set vol_configuration server.ssl on volume set: success Time to allow our nodes to mount the volume as client and brick data as server. # gluster volume set vol_configuration auth.ssl-allow swarm1.lab.local,swarm2.lab.local,swarm3.lab.local volume set: success And now we start it. # gluster volume start vol_configuration volume start: vol_configuration: success Now we need to create a mount point for it. # mkdir /mnt/configuration_data/ # ssh root@swarm2 mkdir /mnt/configuration_data/ # ssh root@swarm3 mkdir /mnt/configuration_data/ And now we mount it. # mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data # ssh root@swarm2 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data # ssh root@swarm3 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data We check this is working as it should: # touch /mnt/configuration_data/youpi # ssh root@swarm2 ls /mnt/configuration_data # ssh root@swarm3 ls /mnt/configuration_data youpi And that ssl is activated grep ssl /var/log/glusterfs/mnt-configuration_data.log We want to have it mounted on startup so we add this to /etc/fstab of each node: echo \"localhost:/vol_configuration /mnt/configuration_data glusterfs defaults,_netdev,noauto,x-systemd.automount,x-systemd.requires=gulsterd.service,x-systemd.after=gulsterd.service 0 0 All the x-systemd option are here to make sure the volume is mounted after glusterFS service is running. For more information read it here https://www.freedesktop.org/software/systemd/man/systemd.mount.html .","title":"Compose and env Volumes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#docker-data-volumes","text":"Same as above but with the brick made here : /var/glusterfs/no-direct-write-here/vol_docker/brick1 And mountpoint: /mnt/docker_data/","title":"Docker Data Volumes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#funny-stuff","text":"Imagine you got your brick (/var/glusterfs/no-direct-write-here/vol_configuration/brick1) and swarm1 on a dedicated hard drive and this hard drive died for some reason. You still could use the hosts to run containers, just mount the volume from an other node mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data You could have also use two bricks or more instead of one to create the GlusterFS volume quite like you will have done it with Raid1. But this is to be read on the Official documentation of the project.","title":"Funny stuff"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#troubleshoot-no-glusterfs-mount-on-startup","text":"I encountered issue for the glusterFS saying a fresh peer couldn't be added because it had existing volumes. This was due to the fact I forgot to recreate the gluster.ca file to add it's public certificate. Official Documentation : https://docs.gluster.org/en/latest/","title":"Troubleshoot no GlusterFS mount on startup"},{"location":"3-nodes-swarm/Stacks/Portainer/","text":"Portainer # Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use. It consists of a single container that can run on any Docker engine (can be deployed as Linux container or a Windows native container, supports other platforms too). Portainer allows you to manage all your Docker resources (containers, images, volumes, networks and more!) It is compatible with the standalone Docker engine and with Docker Swarm mode. Preparaton # Setup Docker Swarm # Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/administration # curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml Customisation # By default portainer stack is using a normal docker volume. So if the node where the container will run fail, then we lost the data. To counter this we are going to use a local file shared accross nodes by GlusterFS. Edit portainer-agent-stack.yml Change: portainer_data:/data for /mnt/docker_data/administration/portainer_data:/data Remove: volumes: portainer_data: The file should look like this (I added some notes as comment): version: '3.2' services: agent: image: portainer/agent volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes networks: - agent_network deploy: mode: global # Will be deploy on all nodes manager or worker accross the swarm. placement: constraints: [node.platform.os == linux] # Will only run on Linux OS. portainer: image: portainer/portainer-ce command: -H tcp://tasks.agent:9001 --tlsskipverify ports: - \"9000:9000\" - \"8000:8000\" volumes: - /mnt/docker_data/administration/portainer_data:/data networks: - agent_network deploy: mode: replicated replicas: 1 # Only one container of this type gonna run at a time. placement: constraints: [node.role == manager] # Will only be deployed on a manager node. networks: agent_network: driver: overlay attachable: true Setup data locations # One issue you gonna have with swarm is that it's not creating folder for declared volume on it's own so you will have to create it by yourself. # mkdir -p /mnt/docker_data/administration/portainer_data If like me you are using the same base directory for docker data, you can run this on the compose file to automatically build the folder. grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. Deploy Portainer stack # Deploy the Portainer stack by running docker stack deploy -c <path -to-docker-compose.yml> portainer Log into your new instance at any nodes on port 9000. You'll be prompted to set your admin user/password on first login. Start at \"Home\", and click on \"Primary\" to manage your swarm (you can manage multiple swarms via one Portainer instance using the agent Todo # Add image Add Some basic usage","title":"Portainer"},{"location":"3-nodes-swarm/Stacks/Portainer/#portainer","text":"Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use. It consists of a single container that can run on any Docker engine (can be deployed as Linux container or a Windows native container, supports other platforms too). Portainer allows you to manage all your Docker resources (containers, images, volumes, networks and more!) It is compatible with the standalone Docker engine and with Docker Swarm mode.","title":"Portainer"},{"location":"3-nodes-swarm/Stacks/Portainer/#preparaton","text":"","title":"Preparaton"},{"location":"3-nodes-swarm/Stacks/Portainer/#setup-docker-swarm","text":"Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/administration # curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml","title":"Setup Docker Swarm"},{"location":"3-nodes-swarm/Stacks/Portainer/#customisation","text":"By default portainer stack is using a normal docker volume. So if the node where the container will run fail, then we lost the data. To counter this we are going to use a local file shared accross nodes by GlusterFS. Edit portainer-agent-stack.yml Change: portainer_data:/data for /mnt/docker_data/administration/portainer_data:/data Remove: volumes: portainer_data: The file should look like this (I added some notes as comment): version: '3.2' services: agent: image: portainer/agent volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes networks: - agent_network deploy: mode: global # Will be deploy on all nodes manager or worker accross the swarm. placement: constraints: [node.platform.os == linux] # Will only run on Linux OS. portainer: image: portainer/portainer-ce command: -H tcp://tasks.agent:9001 --tlsskipverify ports: - \"9000:9000\" - \"8000:8000\" volumes: - /mnt/docker_data/administration/portainer_data:/data networks: - agent_network deploy: mode: replicated replicas: 1 # Only one container of this type gonna run at a time. placement: constraints: [node.role == manager] # Will only be deployed on a manager node. networks: agent_network: driver: overlay attachable: true","title":"Customisation"},{"location":"3-nodes-swarm/Stacks/Portainer/#setup-data-locations","text":"One issue you gonna have with swarm is that it's not creating folder for declared volume on it's own so you will have to create it by yourself. # mkdir -p /mnt/docker_data/administration/portainer_data If like me you are using the same base directory for docker data, you can run this on the compose file to automatically build the folder. grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake.","title":"Setup data locations"},{"location":"3-nodes-swarm/Stacks/Portainer/#deploy-portainer-stack","text":"Deploy the Portainer stack by running docker stack deploy -c <path -to-docker-compose.yml> portainer Log into your new instance at any nodes on port 9000. You'll be prompted to set your admin user/password on first login. Start at \"Home\", and click on \"Primary\" to manage your swarm (you can manage multiple swarms via one Portainer instance using the agent","title":"Deploy Portainer stack"},{"location":"3-nodes-swarm/Stacks/Portainer/#todo","text":"Add image Add Some basic usage","title":"Todo"},{"location":"3-nodes-swarm/Stacks/Shuffler/","text":"Shuffle # Shuffle is an Open Source SOAR I really appreciate as it does focus on CyberSecurity. To know more about it go there: https://medium.com/shuffle-automation Or there : Official Website Hi Frikky !!! Hoipefully, you will appreciate what you are seing here ^^ Preparaton # Setup data locations # Setup Docker Swarm # Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/SOAR Customisation # At the time of this writing I may be the only one shufflying around with swarm :p So there is not yet a stack ready compose file. Let's create one /mnt/configuration_data/compose_files/SOAR/shuffle.stack.yml version: '3.8' services: backend: image: ghcr.io/frikky/shuffle-backend:latest environment: BACKEND_HOSTNAME: backend BACKEND_PORT: '5001' DATASTORE_EMULATOR_HOST: shuffle-database:8000 DB_LOCATION: ./shuffle-database ENVIRONMENT_NAME: Shuffle FRONTEND_PORT: '3001' FRONTEND_PORT_HTTPS: '3443' HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle OUTER_HOSTNAME: backend SHUFFLE_APP_DOWNLOAD_LOCATION: https://github.com/frikky/shuffle-apps SHUFFLE_APP_FORCE_UPDATE: 'false' SHUFFLE_APP_HOTLOAD_FOLDER: /shuffle-apps SHUFFLE_APP_HOTLOAD_LOCATION: ./shuffle-apps SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: '-0.9.30' SHUFFLE_CONTAINER_AUTO_CLEANUP: 'true' SHUFFLE_DEFAULT_APIKEY: '' SHUFFLE_DOWNLOAD_AUTH_BRANCH: '' SHUFFLE_DOWNLOAD_AUTH_PASSWORD: '' SHUFFLE_DOWNLOAD_AUTH_USERNAME: '' SHUFFLE_DOWNLOAD_WORKFLOW_BRANCH: '' SHUFFLE_DOWNLOAD_WORKFLOW_LOCATION: '' SHUFFLE_DOWNLOAD_WORKFLOW_PASSWORD: '' SHUFFLE_DOWNLOAD_WORKFLOW_USERNAME: '' SHUFFLE_ELASTIC: 'true' SHUFFLE_FILE_LOCATION: /shuffle-files SHUFFLE_OPENSEARCH_APIKEY: '' SHUFFLE_OPENSEARCH_CERTIFICATE_FILE: '' SHUFFLE_OPENSEARCH_CLOUDID: '' SHUFFLE_OPENSEARCH_PROXY: '' SHUFFLE_OPENSEARCH_SKIPSSL_VERIFY: 'true' SHUFFLE_OPENSEARCH_URL: http://opensearch:9200 SHUFFLE_PASS_APP_PROXY: 'FALSE' SHUFFLE_PASS_WORKER_PROXY: 'TRUE' SHUFFLE_ENCRYPTION_MODIFIER: a7cee5f9-8e55-4e05-98d4-59aa9595339b BASE_URL: https://shuffle.lab.local SSO_REDIRECT_URL: https://shuffle.lab.local #ports: # - 5001:5001 volumes: - /var/run/docker.sock:/var/run/docker.sock - /mnt/swarm/container_data/Shuffle/shuffle-apps:/shuffle-apps - /mnt/swarm/container_data/Shuffle/shuffle-files:/shuffle-files networks: - shuffle depends_on: - opensearch logging: driver: json-file deploy: placement: constraints: - node.labels.localisation==PROD resources: limits: #cpus: '0.5' memory: 8G frontend: image: ghcr.io/frikky/shuffle-frontend:latest healthcheck: test: curl -fs http://localhost:80 || exit 1 interval: 30s timeout: 5s retries: 3 environment: BACKEND_HOSTNAME: backend BASE_URL: https://shuffle.lab.local SSO_REDIRECT_URL: https://shuffle.lab.local #ports: # - 3001:80 # - 3443:443 networks: - shuffle - reverseproxy logging: driver: json-file depends_on: - backend deploy: placement: constraints: - node.labels.localisation==PROD labels: - \"traefik.enable=true\" - \"traefik.http.routers.shuffle.rule=Host(`shuffle.lab.local`)\" - \"traefik.http.routers.shuffle.entrypoints=web-secure,web\" - \"traefik.http.routers.shuffle.tls=true\" - \"traefik.http.services.shuffle.loadbalancer.server.port=80\" - \"traefik.docker.network=reverseproxy\" opensearch: image: opensearchproject/opensearch:2 healthcheck: test: curl -fs http://localhost:9200/_cat/health || exit 1 interval: 30s timeout: 5s retries: 3 start_period: 45s environment: OPENSEARCH_JAVA_OPTS: -Xms4096m -Xmx4096m bootstrap.memory_lock: 'true' cluster.initial_master_nodes: opensearch node.store.allow_mmap: 'false' cluster.name: shuffle-cluster cluster.routing.allocation.disk.threshold_enabled: 'false' discovery.seed_hosts: opensearch node.name: opensearch plugins.security.disabled: 'true' ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 #ports: #- 9200:9200 volumes: - /mnt/swarm/container_data/Shuffle/shuffle-database:/usr/share/opensearch/data #- /var/glusterfs/shuffle-database:/usr/share/opensearch/data - /mnt/swarm/container_data/Shuffle/shuffle-backup/opensearch:/mnt/backup networks: - shuffle logging: driver: json-file deploy: #endpoint_mode: dnsrr placement: constraints: - node.labels.localisation==PROD resources: limits: #cpus: '0.5' memory: 8G orborus: #image: ghcr.io/frikky/shuffle-orborus:latest image: ghcr.io/frikky/shuffle-orborus:nightly environment: #SHUFFLE_APP_SDK_VERSION: 0.8.97 SHUFFLE_WORKER_VERSION: latest BASE_URL: http://tasks.backend:5001 #BASE_URL: https://shuffle.lab.local CLEANUP: 'true' DOCKER_API_VERSION: '1.41' ENVIRONMENT_NAME: Shuffle HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: -0.8.80 SHUFFLE_ORBORUS_EXECUTION_CONCURRENCY: '50' SHUFFLE_ORBORUS_EXECUTION_TIMEOUT: '800' SHUFFLE_PASS_APP_PROXY: 'FALSE' SHUFFLE_PASS_WORKER_PROXY: 'FALSE' SHUFFLE_SCALE_REPLICAS: 2 SHUFFLE_SWARM_CONFIG: run SHUFFLE_LOGS_DISABLED: \"true\" SHUFFLE_SWARM_NETWORK_NAME: shuffle SSO_REDIRECT_URL: https://shuffle.lab.local volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - shuffle depends_on: - backend logging: driver: json-file deploy: placement: constraints: - node.labels.localisation==PROD oauth2-proxy: container_name: oauth2-proxy image: quay.io/oauth2-proxy/oauth2-proxy:latest command: --config /oauth2-proxy.cfg hostname: oauth2-proxy networks: - shuffle - reverseproxy volumes: - /mnt/swarm/container_data/Shuffle/oauth2-proxy/oauth2-proxy.cfg:/oauth2-proxy.cfg - /mnt/swarm/container_data/Shuffle/oauth2-proxy/logs:/logs - /etc/ssl/certs/:/etc/ssl/certs/:ro restart: unless-stopped deploy: placement: constraints: - node.labels.localisation==PROD labels: - \"traefik.enable=true\" - \"traefik.http.routers.opensearch-secure.rule=Host(`shuffle.lab.local`)\" - \"traefik.http.routers.opensearch-secure.entrypoints=opensearch-secure\" - \"traefik.http.routers.opensearch-secure.tls=true\" - \"traefik.http.services.opensearch-secure.loadbalancer.server.port=4180\" - \"traefik.docker.network=reverseproxy\" dashboards: image: opensearchproject/opensearch-dashboards:2 #ports: #- 5601:5601 environment: OPENSEARCH_HOSTS: '[\"http://opensearch:9200\"]' # must be a string with no spaces when specified as an environment variable DISABLE_INSTALL_DEMO_CONFIG: \"true\" DISABLE_SECURITY_DASHBOARDS_PLUGIN: \"true\" networks: - shuffle networks: shuffle: driver: overlay external: true reverseproxy: driver: overlay external: true Setup data locations # This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. # grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/SOAR/shuffle-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' Deploy Shuffle stack # Deploy the Shuffle stack by running docker stack deploy -c <path -to-docker-compose.yml> shuffle Log into your new instance at any nodes on port https://node:3443. Note # Have you may have seen, all except database have a replica set to 3. This is done so we don't create a messy mess on DB if writing are done from different shuffle-database container at the same time on the same value which will lead to database corruption. Anyway, this container is up in 7 second in case of failure of the node ^^ . App gonna be slow to run on it's first usage on all new nodes where Orborus is going to use the base image for the first time. As swarm distribute request around all replicas, sometime request gonna be fast as the node already run the image and sometime not. This all Replica and Clustering of Shuffle is not yet production ready as it is not yet properly implemented. I recommend using only one database and one Orborus for now.","title":"Shuffle"},{"location":"3-nodes-swarm/Stacks/Shuffler/#shuffle","text":"Shuffle is an Open Source SOAR I really appreciate as it does focus on CyberSecurity. To know more about it go there: https://medium.com/shuffle-automation Or there : Official Website Hi Frikky !!! Hoipefully, you will appreciate what you are seing here ^^","title":"Shuffle"},{"location":"3-nodes-swarm/Stacks/Shuffler/#preparaton","text":"","title":"Preparaton"},{"location":"3-nodes-swarm/Stacks/Shuffler/#setup-data-locations","text":"","title":"Setup data locations"},{"location":"3-nodes-swarm/Stacks/Shuffler/#setup-docker-swarm","text":"Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/SOAR","title":"Setup Docker Swarm"},{"location":"3-nodes-swarm/Stacks/Shuffler/#customisation","text":"At the time of this writing I may be the only one shufflying around with swarm :p So there is not yet a stack ready compose file. Let's create one /mnt/configuration_data/compose_files/SOAR/shuffle.stack.yml version: '3.8' services: backend: image: ghcr.io/frikky/shuffle-backend:latest environment: BACKEND_HOSTNAME: backend BACKEND_PORT: '5001' DATASTORE_EMULATOR_HOST: shuffle-database:8000 DB_LOCATION: ./shuffle-database ENVIRONMENT_NAME: Shuffle FRONTEND_PORT: '3001' FRONTEND_PORT_HTTPS: '3443' HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle OUTER_HOSTNAME: backend SHUFFLE_APP_DOWNLOAD_LOCATION: https://github.com/frikky/shuffle-apps SHUFFLE_APP_FORCE_UPDATE: 'false' SHUFFLE_APP_HOTLOAD_FOLDER: /shuffle-apps SHUFFLE_APP_HOTLOAD_LOCATION: ./shuffle-apps SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: '-0.9.30' SHUFFLE_CONTAINER_AUTO_CLEANUP: 'true' SHUFFLE_DEFAULT_APIKEY: '' SHUFFLE_DOWNLOAD_AUTH_BRANCH: '' SHUFFLE_DOWNLOAD_AUTH_PASSWORD: '' SHUFFLE_DOWNLOAD_AUTH_USERNAME: '' SHUFFLE_DOWNLOAD_WORKFLOW_BRANCH: '' SHUFFLE_DOWNLOAD_WORKFLOW_LOCATION: '' SHUFFLE_DOWNLOAD_WORKFLOW_PASSWORD: '' SHUFFLE_DOWNLOAD_WORKFLOW_USERNAME: '' SHUFFLE_ELASTIC: 'true' SHUFFLE_FILE_LOCATION: /shuffle-files SHUFFLE_OPENSEARCH_APIKEY: '' SHUFFLE_OPENSEARCH_CERTIFICATE_FILE: '' SHUFFLE_OPENSEARCH_CLOUDID: '' SHUFFLE_OPENSEARCH_PROXY: '' SHUFFLE_OPENSEARCH_SKIPSSL_VERIFY: 'true' SHUFFLE_OPENSEARCH_URL: http://opensearch:9200 SHUFFLE_PASS_APP_PROXY: 'FALSE' SHUFFLE_PASS_WORKER_PROXY: 'TRUE' SHUFFLE_ENCRYPTION_MODIFIER: a7cee5f9-8e55-4e05-98d4-59aa9595339b BASE_URL: https://shuffle.lab.local SSO_REDIRECT_URL: https://shuffle.lab.local #ports: # - 5001:5001 volumes: - /var/run/docker.sock:/var/run/docker.sock - /mnt/swarm/container_data/Shuffle/shuffle-apps:/shuffle-apps - /mnt/swarm/container_data/Shuffle/shuffle-files:/shuffle-files networks: - shuffle depends_on: - opensearch logging: driver: json-file deploy: placement: constraints: - node.labels.localisation==PROD resources: limits: #cpus: '0.5' memory: 8G frontend: image: ghcr.io/frikky/shuffle-frontend:latest healthcheck: test: curl -fs http://localhost:80 || exit 1 interval: 30s timeout: 5s retries: 3 environment: BACKEND_HOSTNAME: backend BASE_URL: https://shuffle.lab.local SSO_REDIRECT_URL: https://shuffle.lab.local #ports: # - 3001:80 # - 3443:443 networks: - shuffle - reverseproxy logging: driver: json-file depends_on: - backend deploy: placement: constraints: - node.labels.localisation==PROD labels: - \"traefik.enable=true\" - \"traefik.http.routers.shuffle.rule=Host(`shuffle.lab.local`)\" - \"traefik.http.routers.shuffle.entrypoints=web-secure,web\" - \"traefik.http.routers.shuffle.tls=true\" - \"traefik.http.services.shuffle.loadbalancer.server.port=80\" - \"traefik.docker.network=reverseproxy\" opensearch: image: opensearchproject/opensearch:2 healthcheck: test: curl -fs http://localhost:9200/_cat/health || exit 1 interval: 30s timeout: 5s retries: 3 start_period: 45s environment: OPENSEARCH_JAVA_OPTS: -Xms4096m -Xmx4096m bootstrap.memory_lock: 'true' cluster.initial_master_nodes: opensearch node.store.allow_mmap: 'false' cluster.name: shuffle-cluster cluster.routing.allocation.disk.threshold_enabled: 'false' discovery.seed_hosts: opensearch node.name: opensearch plugins.security.disabled: 'true' ulimits: memlock: soft: -1 hard: -1 nofile: soft: 65536 hard: 65536 #ports: #- 9200:9200 volumes: - /mnt/swarm/container_data/Shuffle/shuffle-database:/usr/share/opensearch/data #- /var/glusterfs/shuffle-database:/usr/share/opensearch/data - /mnt/swarm/container_data/Shuffle/shuffle-backup/opensearch:/mnt/backup networks: - shuffle logging: driver: json-file deploy: #endpoint_mode: dnsrr placement: constraints: - node.labels.localisation==PROD resources: limits: #cpus: '0.5' memory: 8G orborus: #image: ghcr.io/frikky/shuffle-orborus:latest image: ghcr.io/frikky/shuffle-orborus:nightly environment: #SHUFFLE_APP_SDK_VERSION: 0.8.97 SHUFFLE_WORKER_VERSION: latest BASE_URL: http://tasks.backend:5001 #BASE_URL: https://shuffle.lab.local CLEANUP: 'true' DOCKER_API_VERSION: '1.41' ENVIRONMENT_NAME: Shuffle HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: -0.8.80 SHUFFLE_ORBORUS_EXECUTION_CONCURRENCY: '50' SHUFFLE_ORBORUS_EXECUTION_TIMEOUT: '800' SHUFFLE_PASS_APP_PROXY: 'FALSE' SHUFFLE_PASS_WORKER_PROXY: 'FALSE' SHUFFLE_SCALE_REPLICAS: 2 SHUFFLE_SWARM_CONFIG: run SHUFFLE_LOGS_DISABLED: \"true\" SHUFFLE_SWARM_NETWORK_NAME: shuffle SSO_REDIRECT_URL: https://shuffle.lab.local volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - shuffle depends_on: - backend logging: driver: json-file deploy: placement: constraints: - node.labels.localisation==PROD oauth2-proxy: container_name: oauth2-proxy image: quay.io/oauth2-proxy/oauth2-proxy:latest command: --config /oauth2-proxy.cfg hostname: oauth2-proxy networks: - shuffle - reverseproxy volumes: - /mnt/swarm/container_data/Shuffle/oauth2-proxy/oauth2-proxy.cfg:/oauth2-proxy.cfg - /mnt/swarm/container_data/Shuffle/oauth2-proxy/logs:/logs - /etc/ssl/certs/:/etc/ssl/certs/:ro restart: unless-stopped deploy: placement: constraints: - node.labels.localisation==PROD labels: - \"traefik.enable=true\" - \"traefik.http.routers.opensearch-secure.rule=Host(`shuffle.lab.local`)\" - \"traefik.http.routers.opensearch-secure.entrypoints=opensearch-secure\" - \"traefik.http.routers.opensearch-secure.tls=true\" - \"traefik.http.services.opensearch-secure.loadbalancer.server.port=4180\" - \"traefik.docker.network=reverseproxy\" dashboards: image: opensearchproject/opensearch-dashboards:2 #ports: #- 5601:5601 environment: OPENSEARCH_HOSTS: '[\"http://opensearch:9200\"]' # must be a string with no spaces when specified as an environment variable DISABLE_INSTALL_DEMO_CONFIG: \"true\" DISABLE_SECURITY_DASHBOARDS_PLUGIN: \"true\" networks: - shuffle networks: shuffle: driver: overlay external: true reverseproxy: driver: overlay external: true","title":"Customisation"},{"location":"3-nodes-swarm/Stacks/Shuffler/#setup-data-locations_1","text":"This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. # grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/SOAR/shuffle-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }'","title":"Setup data locations"},{"location":"3-nodes-swarm/Stacks/Shuffler/#deploy-shuffle-stack","text":"Deploy the Shuffle stack by running docker stack deploy -c <path -to-docker-compose.yml> shuffle Log into your new instance at any nodes on port https://node:3443.","title":"Deploy Shuffle stack"},{"location":"3-nodes-swarm/Stacks/Shuffler/#note","text":"Have you may have seen, all except database have a replica set to 3. This is done so we don't create a messy mess on DB if writing are done from different shuffle-database container at the same time on the same value which will lead to database corruption. Anyway, this container is up in 7 second in case of failure of the node ^^ . App gonna be slow to run on it's first usage on all new nodes where Orborus is going to use the base image for the first time. As swarm distribute request around all replicas, sometime request gonna be fast as the node already run the image and sometime not. This all Replica and Clustering of Shuffle is not yet production ready as it is not yet properly implemented. I recommend using only one database and one Orborus for now.","title":"Note"},{"location":"Thanks/Sponsors_and_Higlights/","text":"Share and Rise Community # Thanks to all of the following I am writing those pages today. People # Penthium2: one teacher of a kind. # Community # BZHack : https://www.bzhack.bzh/ # BZHack is an association, born from a group of geeks passionate about computers, security and hacking Projects # Shuffle : https://shuffler.io/ # Shuffle has made workflows that are available to anyone. These workflows are made using some of our favorite frameworks and tools, and are designed to help you get started with Shuffle using your own tools. Here are some samples. Thehive-project : https://thehive-project.org/ # A scalable, open source and free Security Incident Response Platform, tightly integrated with MISP (Malware Information Sharing Platform), designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly.","title":"Share and Rise Community"},{"location":"Thanks/Sponsors_and_Higlights/#share-and-rise-community","text":"Thanks to all of the following I am writing those pages today.","title":"Share and Rise Community"},{"location":"Thanks/Sponsors_and_Higlights/#people","text":"","title":"People"},{"location":"Thanks/Sponsors_and_Higlights/#penthium2-one-teacher-of-a-kind","text":"","title":"Penthium2: one teacher of a kind."},{"location":"Thanks/Sponsors_and_Higlights/#community","text":"","title":"Community"},{"location":"Thanks/Sponsors_and_Higlights/#bzhack-httpswwwbzhackbzh","text":"BZHack is an association, born from a group of geeks passionate about computers, security and hacking","title":"BZHack : https://www.bzhack.bzh/"},{"location":"Thanks/Sponsors_and_Higlights/#projects","text":"","title":"Projects"},{"location":"Thanks/Sponsors_and_Higlights/#shuffle-httpsshufflerio","text":"Shuffle has made workflows that are available to anyone. These workflows are made using some of our favorite frameworks and tools, and are designed to help you get started with Shuffle using your own tools. Here are some samples.","title":"Shuffle : https://shuffler.io/"},{"location":"Thanks/Sponsors_and_Higlights/#thehive-project-httpsthehive-projectorg","text":"A scalable, open source and free Security Incident Response Platform, tightly integrated with MISP (Malware Information Sharing Platform), designed to make life easier for SOCs, CSIRTs, CERTs and any information security practitioner dealing with security incidents that need to be investigated and acted upon swiftly.","title":"Thehive-project : https://thehive-project.org/"}]}