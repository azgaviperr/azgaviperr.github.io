{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WEBSITE UNDER CONSTRUCTION TODO # Create content Create moaarr content","title":"Home"},{"location":"#todo","text":"Create content Create moaarr content","title":"TODO"},{"location":"3-nodes-swarm/","text":"# Docker the Zerg way # For a resilient installation we are going to build a docker swarm of 3 master nodes with also a worker role. (In swarm node can be master or worker - the one running containers - or both) It's important to understand the split brain concept. In the following cluster a quorum have to be kept to assure things don't go sideways. For the quorum to be achieve you should have 2 master nodes up. This is true for Swarm and GlusterFS. Nodes gonna be called: swarm1, swarm2, swarm3 +----------------------+ | +----------------------+ | [Node #1] |10.0.0.51 | | | [Node #1] |10.0.0.52 | | swarm1.lab.local +----------+----------+ swarm2.lab.local | | | | | | +----------------------+ | +----------------------+ | | | +----------------------+ | | [Node #3] |10.0.0.53 | | | swarm3.lab.local +----------+ | | +----------------------+ Why Swarm ? Swarm is an orchestration tool directly provided into docker from the version 1.12. It is easy to use compare to Kubernetes and easier to maintain for a small team. Highly-available (can tolerate the failure of a single component) Scalable (can add resource or capacity as required) Portable (run it on your home today, run it in everywhere tomorrow) Automated (requires minimal care and feeding) Why GlusterFS ? While Docker Swarm is great for keeping containers running and providing scaling capabilities, it does lack direct integration of persistent storage accross nodes. This means if you actually want your containers to keep any data persistent across restarts of services, you need to provide a shared storage to every docker nodes. This also means you shouldn't use docker volume declaration in you docker files. Installing the host component # Installation is based on a fresh Centos Stream minimal server and should be executed on all nodes.","title":"Architecture"},{"location":"3-nodes-swarm/#_1","text":"","title":""},{"location":"3-nodes-swarm/#docker-the-zerg-way","text":"For a resilient installation we are going to build a docker swarm of 3 master nodes with also a worker role. (In swarm node can be master or worker - the one running containers - or both) It's important to understand the split brain concept. In the following cluster a quorum have to be kept to assure things don't go sideways. For the quorum to be achieve you should have 2 master nodes up. This is true for Swarm and GlusterFS. Nodes gonna be called: swarm1, swarm2, swarm3 +----------------------+ | +----------------------+ | [Node #1] |10.0.0.51 | | | [Node #1] |10.0.0.52 | | swarm1.lab.local +----------+----------+ swarm2.lab.local | | | | | | +----------------------+ | +----------------------+ | | | +----------------------+ | | [Node #3] |10.0.0.53 | | | swarm3.lab.local +----------+ | | +----------------------+ Why Swarm ? Swarm is an orchestration tool directly provided into docker from the version 1.12. It is easy to use compare to Kubernetes and easier to maintain for a small team. Highly-available (can tolerate the failure of a single component) Scalable (can add resource or capacity as required) Portable (run it on your home today, run it in everywhere tomorrow) Automated (requires minimal care and feeding) Why GlusterFS ? While Docker Swarm is great for keeping containers running and providing scaling capabilities, it does lack direct integration of persistent storage accross nodes. This means if you actually want your containers to keep any data persistent across restarts of services, you need to provide a shared storage to every docker nodes. This also means you shouldn't use docker volume declaration in you docker files.","title":"Docker the Zerg way"},{"location":"3-nodes-swarm/#installing-the-host-component","text":"Installation is based on a fresh Centos Stream minimal server and should be executed on all nodes.","title":"Installing the host component"},{"location":"3-nodes-swarm/DockerSwarm/Installation/","text":"Docker swarm # Installing Docker and Docker compose # Remove runc # # dnf remove runc Adding docker repo # # dnf install -y yum-utils # dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # dnf install docker-ce docker-ce-cli containerd.io Installing docker-compose # dnf install python3-pip pip3 install --upgrade pip pip3 install setuptools-rust pip3 install docker-compose Let's start the whale # systemctl enable docker --now And now create a zerg warm from it # From swarm1: # docker swarm init Swarm initialized: current node ( ksdjlqsldjqsd2516685485 ) is now a manager. To add a worker to this swarm, run the following command: docker swarm \\ join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-814yer1z55vmyk2mwdhvjbob1 \\ 10 .0.0.51:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. We are going to add the two other nodes as manager: docker swarm join-token manager To add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-2k0vay9aub5eheikw7qi9v82o 10 .0.0.51:2377 Run the command provided on your other nodes to join them to the swarm as managers. After addition of a node, the output of docker node ls (on either host) should reflect all the nodes: # docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION p424u0yvmu0vvc8nsnspv83zw * swarm1.lab.local Ready Active Leader 20 .10.6 kg6w6ucpb2jf8v8xqai23pv3a swarm2.lab.local Ready Active Reachable 20 .10.6 lam7mgs5wus40iaydvp8u3ss7 swarm3.lab.local Ready Active Reachable 20 .10.6 You are now ready to swarm. Official Documentation : https://docs.docker.com/engine/swarm/ Little Network tweak # In some heavy network swarm, service can show some timeout connections. To fix this on CentOS this little trick will do: ethtool -K ens192 tx-checksum-ip-generic off cat > /etc/NetworkManager/dispatcher.d/pre-up.d/10-tx-checksum-ip-generic <<'EOF' ethtool -K ens192 tx-checksum-ip-generic off EOF chmod +x /etc/NetworkManager/dispatcher.d/pre-up.d/10-tx-checksum-ip-generic Note: ens192 is your network adaptater so change it accordingly.","title":"Docker swarm"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#docker-swarm","text":"","title":"Docker swarm"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#installing-docker-and-docker-compose","text":"","title":"Installing Docker and Docker compose"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#remove-runc","text":"# dnf remove runc","title":"Remove runc"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#adding-docker-repo","text":"# dnf install -y yum-utils # dnf config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # dnf install docker-ce docker-ce-cli containerd.io","title":"Adding docker repo"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#installing-docker-compose","text":"dnf install python3-pip pip3 install --upgrade pip pip3 install setuptools-rust pip3 install docker-compose","title":"Installing docker-compose"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#lets-start-the-whale","text":"systemctl enable docker --now","title":"Let's start the whale"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#and-now-create-a-zerg-warm-from-it","text":"From swarm1: # docker swarm init Swarm initialized: current node ( ksdjlqsldjqsd2516685485 ) is now a manager. To add a worker to this swarm, run the following command: docker swarm \\ join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-814yer1z55vmyk2mwdhvjbob1 \\ 10 .0.0.51:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. We are going to add the two other nodes as manager: docker swarm join-token manager To add a manager to this swarm, run the following command: docker swarm join --token SWMTKN-1-5pykfhyfvtsij0tg4ewrtqk7hz2twuq21okeqv54p1gw2ufdde-2k0vay9aub5eheikw7qi9v82o 10 .0.0.51:2377 Run the command provided on your other nodes to join them to the swarm as managers. After addition of a node, the output of docker node ls (on either host) should reflect all the nodes: # docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION p424u0yvmu0vvc8nsnspv83zw * swarm1.lab.local Ready Active Leader 20 .10.6 kg6w6ucpb2jf8v8xqai23pv3a swarm2.lab.local Ready Active Reachable 20 .10.6 lam7mgs5wus40iaydvp8u3ss7 swarm3.lab.local Ready Active Reachable 20 .10.6 You are now ready to swarm. Official Documentation : https://docs.docker.com/engine/swarm/","title":"And now create a zerg warm from it"},{"location":"3-nodes-swarm/DockerSwarm/Installation/#little-network-tweak","text":"In some heavy network swarm, service can show some timeout connections. To fix this on CentOS this little trick will do: ethtool -K ens192 tx-checksum-ip-generic off cat > /etc/NetworkManager/dispatcher.d/pre-up.d/10-tx-checksum-ip-generic <<'EOF' ethtool -K ens192 tx-checksum-ip-generic off EOF chmod +x /etc/NetworkManager/dispatcher.d/pre-up.d/10-tx-checksum-ip-generic Note: ens192 is your network adaptater so change it accordingly.","title":"Little Network tweak"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/","text":"Portainer # Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use. It consists of a single container that can run on any Docker engine (can be deployed as Linux container or a Windows native container, supports other platforms too). Portainer allows you to manage all your Docker resources (containers, images, volumes, networks and more!) It is compatible with the standalone Docker engine and with Docker Swarm mode. Preparaton # Setup Docker Swarm # Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/administration # curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml Customisation # By default portainer stack is using a normal docker volume. So if the node where the container will run fail, then we lost the data. To counter this we are going to use a local file shared accross nodes by GlusterFS. Edit portainer-agent-stack.yml Change: portainer_data:/data for /mnt/docker_data/administration/portainer_data:/data Remove: volumes: portainer_data: The file should look like this (I added some notes as comment): version: '3.2' services: agent: image: portainer/agent volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes networks: - agent_network deploy: mode: global # Will be deploy on all nodes manager or worker accross the swarm. placement: constraints: [node.platform.os == linux] # Will only run on Linux OS. portainer: image: portainer/portainer-ce command: -H tcp://tasks.agent:9001 --tlsskipverify ports: - \"9000:9000\" - \"8000:8000\" volumes: - /mnt/docker_data/administration/portainer_data:/data networks: - agent_network deploy: mode: replicated replicas: 1 # Only one container of this type gonna run at a time. placement: constraints: [node.role == manager] # Will only be deployed on a manager node. networks: agent_network: driver: overlay attachable: true Setup data locations # One issue you gonna have with swarm is that it's not creating folder for declared volume on it's own so you will have to create it by yourself. # mkdir -p /mnt/docker_data/administration/portainer_data If like me you are using the same base directory for docker data, you can run this on the compose file to automatically build the folder. grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. Deploy Portainer stack # Deploy the Portainer stack by running docker stack deploy -c <path -to-docker-compose.yml> portainer Log into your new instance at any nodes on port 9000. You'll be prompted to set your admin user/password on first login. Start at \"Home\", and click on \"Primary\" to manage your swarm (you can manage multiple swarms via one Portainer instance using the agent Todo # Add image Add Some basic usage","title":"Portainer"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#portainer","text":"Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use. It consists of a single container that can run on any Docker engine (can be deployed as Linux container or a Windows native container, supports other platforms too). Portainer allows you to manage all your Docker resources (containers, images, volumes, networks and more!) It is compatible with the standalone Docker engine and with Docker Swarm mode.","title":"Portainer"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#preparaton","text":"","title":"Preparaton"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#setup-docker-swarm","text":"Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/administration # curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml","title":"Setup Docker Swarm"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#customisation","text":"By default portainer stack is using a normal docker volume. So if the node where the container will run fail, then we lost the data. To counter this we are going to use a local file shared accross nodes by GlusterFS. Edit portainer-agent-stack.yml Change: portainer_data:/data for /mnt/docker_data/administration/portainer_data:/data Remove: volumes: portainer_data: The file should look like this (I added some notes as comment): version: '3.2' services: agent: image: portainer/agent volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes networks: - agent_network deploy: mode: global # Will be deploy on all nodes manager or worker accross the swarm. placement: constraints: [node.platform.os == linux] # Will only run on Linux OS. portainer: image: portainer/portainer-ce command: -H tcp://tasks.agent:9001 --tlsskipverify ports: - \"9000:9000\" - \"8000:8000\" volumes: - /mnt/docker_data/administration/portainer_data:/data networks: - agent_network deploy: mode: replicated replicas: 1 # Only one container of this type gonna run at a time. placement: constraints: [node.role == manager] # Will only be deployed on a manager node. networks: agent_network: driver: overlay attachable: true","title":"Customisation"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#setup-data-locations","text":"One issue you gonna have with swarm is that it's not creating folder for declared volume on it's own so you will have to create it by yourself. # mkdir -p /mnt/docker_data/administration/portainer_data If like me you are using the same base directory for docker data, you can run this on the compose file to automatically build the folder. grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake.","title":"Setup data locations"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#deploy-portainer-stack","text":"Deploy the Portainer stack by running docker stack deploy -c <path -to-docker-compose.yml> portainer Log into your new instance at any nodes on port 9000. You'll be prompted to set your admin user/password on first login. Start at \"Home\", and click on \"Primary\" to manage your swarm (you can manage multiple swarms via one Portainer instance using the agent","title":"Deploy Portainer stack"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Portainer/#todo","text":"Add image Add Some basic usage","title":"Todo"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/","text":"Shuffle # Shuffle is an Open Source SOAR I really appreciate as it does focus on CyberSecurity. To know more about it go there: https://medium.com/shuffle-automation Or there : Official Website Hi Frikky !!! Hoipefully, you will appreciate what you are seing here ^^ Preparaton # Setup data locations # Setup Docker Swarm # Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/SOAR Customisation # At the time of this writing I may be the only one shufflying around with swarm :p So there is not yet a stack ready compose file. Let's create one /mnt/configuration_data/compose_files/SOAR/shuffle.stack.yml version: '3.3' services: backend: image: ghcr.io/frikky/shuffle-backend:0.8.74 environment: DATASTORE_EMULATOR_HOST: shuffle-database:8000 HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_APP_DOWNLOAD_LOCATION: https://github.com/frikky/shuffle-apps SHUFFLE_APP_FORCE_UPDATE: 'false' SHUFFLE_APP_HOTLOAD_FOLDER: /shuffle-apps SHUFFLE_DEFAULT_APIKEY: zaCELgL.0imfnc8mVLWwsAawjYr4Rx-Af50DDqtlx SHUFFLE_DEFAULT_PASSWORD: Fr1kky1sN0t@D0g SHUFFLE_DEFAULT_USERNAME: admin SHUFFLE_DOWNLOAD_AUTH_BRANCH: '' SHUFFLE_FILE_LOCATION: /shuffle-files ports: - 5001:5001 volumes: - /var/run/docker.sock:/var/run/docker.sock - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-apps:/shuffle-apps - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-files:/shuffle-files networks: - shuffle logging: driver: json-file deploy: replicas: 3 database: image: frikky/shuffle:database ports: - 30002:8000 volumes: - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-database:/etc/shuffle networks: - shuffle logging: driver: json-file deploy: replicas: 1 frontend: image: ghcr.io/frikky/shuffle-frontend:0.8.74 environment: BACKEND_HOSTNAME: shuffle-backend ports: - 3001:80 - 3443:443 networks: - shuffle logging: driver: json-file deploy: replicas: 3 orborus: image: ghcr.io/frikky/shuffle-orborus:0.8.73 environment: BASE_URL: http://shuffle-backend:5001 CLEANUP: 'false' DOCKER_API_VERSION: '1.40' ENVIRONMENT_NAME: Shuffle HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_APP_SDK_VERSION: 0.8.77 SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: -0.8.60 SHUFFLE_ORBORUS_EXECUTION_TIMEOUT: '600' SHUFFLE_PASS_WORKER_PROXY: 'TRUE' SHUFFLE_WORKER_VERSION: 0.8.73 volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - shuffle logging: driver: json-file deploy: replicas: 3 networks: shuffle: driver: overlay Setup data locations # This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. # grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/SOAR/shuffle-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' Deploy Shuffle stack # Deploy the Shuffle stack by running docker stack deploy -c <path -to-docker-compose.yml> shuffle Log into your new instance at any nodes on port https://node:3443. Note # Have you may have seen, all except database have a replica set to 3. This is done so we don't create a messy mess on DB if writing are intented from different shuffle-database container at the same time on the same value which will lead to databse corruption. Anyway, this container is up in 7 second in case of failure of the node ^^ . App gonna be slow to run on it's first usage on all new nodes where Orborus is going to use the base image for the first time. As swarm distribute request around all replicas, sometime request gonna be fast as the node already run the image and sometime not. This all Replica and Clustering of Shuffle is not yet production ready as it is not yet properly implemented. I recommend using only one database and one Orborus for now.","title":"Shuffle"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#shuffle","text":"Shuffle is an Open Source SOAR I really appreciate as it does focus on CyberSecurity. To know more about it go there: https://medium.com/shuffle-automation Or there : Official Website Hi Frikky !!! Hoipefully, you will appreciate what you are seing here ^^","title":"Shuffle"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#preparaton","text":"","title":"Preparaton"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#setup-data-locations","text":"","title":"Setup data locations"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#setup-docker-swarm","text":"Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/SOAR","title":"Setup Docker Swarm"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#customisation","text":"At the time of this writing I may be the only one shufflying around with swarm :p So there is not yet a stack ready compose file. Let's create one /mnt/configuration_data/compose_files/SOAR/shuffle.stack.yml version: '3.3' services: backend: image: ghcr.io/frikky/shuffle-backend:0.8.74 environment: DATASTORE_EMULATOR_HOST: shuffle-database:8000 HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_APP_DOWNLOAD_LOCATION: https://github.com/frikky/shuffle-apps SHUFFLE_APP_FORCE_UPDATE: 'false' SHUFFLE_APP_HOTLOAD_FOLDER: /shuffle-apps SHUFFLE_DEFAULT_APIKEY: zaCELgL.0imfnc8mVLWwsAawjYr4Rx-Af50DDqtlx SHUFFLE_DEFAULT_PASSWORD: Fr1kky1sN0t@D0g SHUFFLE_DEFAULT_USERNAME: admin SHUFFLE_DOWNLOAD_AUTH_BRANCH: '' SHUFFLE_FILE_LOCATION: /shuffle-files ports: - 5001:5001 volumes: - /var/run/docker.sock:/var/run/docker.sock - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-apps:/shuffle-apps - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-files:/shuffle-files networks: - shuffle logging: driver: json-file deploy: replicas: 3 database: image: frikky/shuffle:database ports: - 30002:8000 volumes: - /mnt/docker_data/SOAR/shuffle_data/shuffle-demo/shuffle-database:/etc/shuffle networks: - shuffle logging: driver: json-file deploy: replicas: 1 frontend: image: ghcr.io/frikky/shuffle-frontend:0.8.74 environment: BACKEND_HOSTNAME: shuffle-backend ports: - 3001:80 - 3443:443 networks: - shuffle logging: driver: json-file deploy: replicas: 3 orborus: image: ghcr.io/frikky/shuffle-orborus:0.8.73 environment: BASE_URL: http://shuffle-backend:5001 CLEANUP: 'false' DOCKER_API_VERSION: '1.40' ENVIRONMENT_NAME: Shuffle HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_APP_SDK_VERSION: 0.8.77 SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: -0.8.60 SHUFFLE_ORBORUS_EXECUTION_TIMEOUT: '600' SHUFFLE_PASS_WORKER_PROXY: 'TRUE' SHUFFLE_WORKER_VERSION: 0.8.73 volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - shuffle logging: driver: json-file deploy: replicas: 3 networks: shuffle: driver: overlay","title":"Customisation"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#setup-data-locations_1","text":"This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. # grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/SOAR/shuffle-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }'","title":"Setup data locations"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#deploy-shuffle-stack","text":"Deploy the Shuffle stack by running docker stack deploy -c <path -to-docker-compose.yml> shuffle Log into your new instance at any nodes on port https://node:3443.","title":"Deploy Shuffle stack"},{"location":"3-nodes-swarm/DockerSwarm/Stacks/Shuffler/#note","text":"Have you may have seen, all except database have a replica set to 3. This is done so we don't create a messy mess on DB if writing are intented from different shuffle-database container at the same time on the same value which will lead to databse corruption. Anyway, this container is up in 7 second in case of failure of the node ^^ . App gonna be slow to run on it's first usage on all new nodes where Orborus is going to use the base image for the first time. As swarm distribute request around all replicas, sometime request gonna be fast as the node already run the image and sometime not. This all Replica and Clustering of Shuffle is not yet production ready as it is not yet properly implemented. I recommend using only one database and one Orborus for now.","title":"Note"},{"location":"3-nodes-swarm/Firewalling/Firewalling/","text":"Firewalling # Base configuration # Activate firewalld, you may want to check in /etc/firewalld/zones/ to check what is going to happen ^^ . One issue happening quite often is when you changed the default ssh port. As the ssh service declared in /usr/lib/firewalld/services/ssh.xml is referencing to port 22. If it's the case, copy the service.xml into /etc/firewalld/service and change the port of it. (And yes, this happen to me a few times) systemctl enable firewalld --now Unmask the service if needed : systemctl unmask firewalld By default, firewalld is having a public zone created. This public zone allow the use of ssh, cockpit, dhcpv6-client. cat /etc/firewalld/zones/public.xml <?xml version = \"1.0\" encoding = \"utf-8\" ?> <zone> <short>Public</short> <description>For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted.</description> <service name = \"ssh\" /> <service name = \"dhcpv6-client\" /> <service name = \"cockpit\" /> </zone If you don't use cockpit or dhcpv6-client, you can remove them from the configuration. For example to delete cockpit service: firewall-cmd --permamnent --zone = public --remove-service = cockpit firewall-cmd --reload Note here the parameters: --permanent: means the rules gonna last after service restart --zone: is used to indicate what zone should be modified, by default it's the public one --remove-service : remove the service declared in /etc/firewalld/services/ without the .xml ending firewall-cmd --reload is going to reload firewalld with latest configuration. Let's add some services firewall-cmd --permanent --zone = public --add-service = http firewall-cmd --permanent --zone = public --add-service = https firewall-cmd --reload We are going now to create a new zone representing the nodes of our cluster, and add sources to it (understand incoming traffic). firewall-cmd --permanent --new-zone = swarm firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.51 firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.52 firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.53 firewall-cmd --reload Let's check if sources where added firewall-cmd --zone = swarm --list-sources 10 .0.0.51 10 .0.0.52 10 .0.0.53 firewall-cmd --zone=swarm --list-sources Ajouter les services n\u00e9cessaires au cluster cp /usr/lib/firewalld/services/docker-swarm.xml /etc/firewalld/services/ firewall-cmd --zone=swarm --add-service=docker-swarm --permanent firewall-cmd --reload Docker Swarm # Let's add the service to the swarm zone: By default firewalld come bundled with some services. You can find them in /usr/lib/firewalld/services\". I like to copy them in /etc/firewalld/services when I use them as it prevent it to be changed after an update. Firewalld prioritize service in \"/etc/firewalld/services/\" then in \"/usr/lib/firewalld/services\" . So let's copy the docker-swarm service: cp /usr/lib/firewalld/services/docker-swarm.xml /etc/firewalld/services/ cat /etc/firewalld/services/docker-swarm.xml <?xml version = \"1.0\" encoding = \"utf-8\" ?> <service> <short>Docker integrated swarm mode</short> <description>Natively managed cluster of Docker Engines ( > = 1 .12.0 ) , where you deploy services.</description> <port port = \"2377\" protocol = \"tcp\" /> <port port = \"7946\" protocol = \"tcp\" /> <port port = \"7946\" protocol = \"udp\" /> <port port = \"4789\" protocol = \"udp\" /> <protocol value = \"esp\" /> </service> Then add it to the zone: firewall-cmd --permanent --zone = swarm --add-service = docker-swarm firewall-cmd --reload firewall-cmd --zone = swarm --list-services docker-swarm Now we did allow the port and protocols for the nodes to be used our cluster, and off course all those action have to be done on each host of the cluster. GlusterFS # By default GlusterFS service for firewalld is not available, but after installing GlusterFS Server on your host, the XML file gonna be available at \"/usr/lib/firewalld/services/\". cat /usr/lib/firewalld/services/glusterfs.xml <?xml version = \"1.0\" encoding = \"utf-8\" ?> <service> <short>glusterfs-static</short> <description>Default ports for gluster-distributed storage</description> <port protocol = \"tcp\" port = \"24007\" /> <!--For glusterd --> <port protocol = \"tcp\" port = \"24008\" /> <!--For glusterd RDMA port management --> <port protocol = \"tcp\" port = \"24009\" /> <!--For glustereventsd --> <port protocol = \"tcp\" port = \"38465\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"38466\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"38467\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"38468\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"38469\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"49152-49664\" /> <!--512 ports for bricks --> </service> cp /usr/lib/firewalld/services/glusterfs.xml /etc/firewalld/services/ firewall-cmd --permanent --zone = swarm --add-service = glusterfs firewall-cmd --reload firewall-cmd --zone = swarm --list-services docker-swarm glusterfs","title":"Firewalling"},{"location":"3-nodes-swarm/Firewalling/Firewalling/#firewalling","text":"","title":"Firewalling"},{"location":"3-nodes-swarm/Firewalling/Firewalling/#base-configuration","text":"Activate firewalld, you may want to check in /etc/firewalld/zones/ to check what is going to happen ^^ . One issue happening quite often is when you changed the default ssh port. As the ssh service declared in /usr/lib/firewalld/services/ssh.xml is referencing to port 22. If it's the case, copy the service.xml into /etc/firewalld/service and change the port of it. (And yes, this happen to me a few times) systemctl enable firewalld --now Unmask the service if needed : systemctl unmask firewalld By default, firewalld is having a public zone created. This public zone allow the use of ssh, cockpit, dhcpv6-client. cat /etc/firewalld/zones/public.xml <?xml version = \"1.0\" encoding = \"utf-8\" ?> <zone> <short>Public</short> <description>For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted.</description> <service name = \"ssh\" /> <service name = \"dhcpv6-client\" /> <service name = \"cockpit\" /> </zone If you don't use cockpit or dhcpv6-client, you can remove them from the configuration. For example to delete cockpit service: firewall-cmd --permamnent --zone = public --remove-service = cockpit firewall-cmd --reload Note here the parameters: --permanent: means the rules gonna last after service restart --zone: is used to indicate what zone should be modified, by default it's the public one --remove-service : remove the service declared in /etc/firewalld/services/ without the .xml ending firewall-cmd --reload is going to reload firewalld with latest configuration. Let's add some services firewall-cmd --permanent --zone = public --add-service = http firewall-cmd --permanent --zone = public --add-service = https firewall-cmd --reload We are going now to create a new zone representing the nodes of our cluster, and add sources to it (understand incoming traffic). firewall-cmd --permanent --new-zone = swarm firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.51 firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.52 firewall-cmd --permanent --zone = swarm --add-source = 10 .0.0.53 firewall-cmd --reload Let's check if sources where added firewall-cmd --zone = swarm --list-sources 10 .0.0.51 10 .0.0.52 10 .0.0.53 firewall-cmd --zone=swarm --list-sources Ajouter les services n\u00e9cessaires au cluster cp /usr/lib/firewalld/services/docker-swarm.xml /etc/firewalld/services/ firewall-cmd --zone=swarm --add-service=docker-swarm --permanent firewall-cmd --reload","title":"Base configuration"},{"location":"3-nodes-swarm/Firewalling/Firewalling/#docker-swarm","text":"Let's add the service to the swarm zone: By default firewalld come bundled with some services. You can find them in /usr/lib/firewalld/services\". I like to copy them in /etc/firewalld/services when I use them as it prevent it to be changed after an update. Firewalld prioritize service in \"/etc/firewalld/services/\" then in \"/usr/lib/firewalld/services\" . So let's copy the docker-swarm service: cp /usr/lib/firewalld/services/docker-swarm.xml /etc/firewalld/services/ cat /etc/firewalld/services/docker-swarm.xml <?xml version = \"1.0\" encoding = \"utf-8\" ?> <service> <short>Docker integrated swarm mode</short> <description>Natively managed cluster of Docker Engines ( > = 1 .12.0 ) , where you deploy services.</description> <port port = \"2377\" protocol = \"tcp\" /> <port port = \"7946\" protocol = \"tcp\" /> <port port = \"7946\" protocol = \"udp\" /> <port port = \"4789\" protocol = \"udp\" /> <protocol value = \"esp\" /> </service> Then add it to the zone: firewall-cmd --permanent --zone = swarm --add-service = docker-swarm firewall-cmd --reload firewall-cmd --zone = swarm --list-services docker-swarm Now we did allow the port and protocols for the nodes to be used our cluster, and off course all those action have to be done on each host of the cluster.","title":"Docker Swarm"},{"location":"3-nodes-swarm/Firewalling/Firewalling/#glusterfs","text":"By default GlusterFS service for firewalld is not available, but after installing GlusterFS Server on your host, the XML file gonna be available at \"/usr/lib/firewalld/services/\". cat /usr/lib/firewalld/services/glusterfs.xml <?xml version = \"1.0\" encoding = \"utf-8\" ?> <service> <short>glusterfs-static</short> <description>Default ports for gluster-distributed storage</description> <port protocol = \"tcp\" port = \"24007\" /> <!--For glusterd --> <port protocol = \"tcp\" port = \"24008\" /> <!--For glusterd RDMA port management --> <port protocol = \"tcp\" port = \"24009\" /> <!--For glustereventsd --> <port protocol = \"tcp\" port = \"38465\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"38466\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"38467\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"38468\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"38469\" /> <!--Gluster NFS service --> <port protocol = \"tcp\" port = \"49152-49664\" /> <!--512 ports for bricks --> </service> cp /usr/lib/firewalld/services/glusterfs.xml /etc/firewalld/services/ firewall-cmd --permanent --zone = swarm --add-service = glusterfs firewall-cmd --reload firewall-cmd --zone = swarm --list-services docker-swarm glusterfs","title":"GlusterFS"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/","text":"Installing GlusterFS Server # Adding Gluster and other repository # dnf install centos-release-gluster dnf install epel-release dnf install dnf-plugins-core dnf config-manager --set-enabled powertools # checking repository list dnf repolist repo id repo name appstream CentOS Stream 8 - AppStream baseos CentOS Stream 8 - BaseOS centos-gluster9 CentOS-8-stream - Gluster 9 epel Extra Packages for Enterprise Linux 8 - x86_64 epel-modular Extra Packages for Enterprise Linux Modular 8 - x86_64 epel-next Extra Packages for Enterprise Linux 8 - Next - x86_64 extras CentOS Stream 8 - Extras powertools CentOS Stream 8 - PowerTools Installing GlusterFS Server # dnf install glusterfs-server A bit of simplification for management. # Generate ssh-key # ssh-keygen -t ed25519 Push ssh-key to all nodes # ssh-copy-id root@swarm2 ssh-copy-id root@swarm3 This should be done on all nodes. Stop firewall # I highly doesn't recommend to stop the firewall but I don't want also that you will get frustrated because of it. So go look how to stop firewalld on CentOS using your favorite search engine. Or read the Firewalling documentation of this website. Configure GlusterFS with TLS # The fact that we gonna use certificate means you will have to take care to it and specially the expiration date. If certificate expire, GlusterFS service is not going to work. There is way to handle automatic renewall of certificate but this is not going to be part of this article. generate a private key for each node # openssl genrsa -out /etc/ssl/glusterfs.key 4096 generate a signed certificate for each node # openssl req -new -x509 -key /etc/ssl/glusterfs.key -subj \"/CN=`hostname -f`\" -out /etc/ssl/glusterfs.pem hostname -f : for full fqdn (swarm1.lab.local) hostname -s : for short version (swarm1) This is important to understand what are you going to use on all the nodes as it's going to be used to identify the allowed host to mount Gluster's volume. Oh! Before I forget, it's good to check the expiration date of the certificate :) . By default it expire after 30 days. Create the gluster.ca # To be able to use TLS with Gluster Server/Client you need to create /etc/ssl/glusterfs.ca concatenation of others' certificates . The easiest way it's to create a gluster.ca with all three .pem in it and set it up on each nodes. GlusterFS always performs mutual authentication , though clients do not currently do anything with the authenticated server identity. Thus, if client X wants to communicate with server Y, then X's certificate (or that of a signer) must be in Y's CA file, and vice versa. For all uses of TLS in GlusterFS, if one side of a connection is configured to use TLS then the other side must use it as well. There is no automatic fallback to non-TLS communication, or allowance for concurrent TLS and non-TLS access to the same resource, because either would be insecure. Instead, any such \"mixed mode\" connections will be rejected by the TLS-using side, sacrificing availability to maintain security. NOTE : The TLS certificate verification will fail if the machines' date and time are not in sync with each other. Certificate verification depends on the time of the client as well as the server and if that is not found to be in sync then it is deemed to be an invalid certificate. To get the date and times in sync, tools such as ntpdate can be used. Enable and start service # systemctl enable glusterd --now Peer nodes # From swarm1 run : gluster peer probe swarm2 gluster peer probe swarm3 Verify peers: gluster peer status Number of Peers: 2 Hostname: swarm2 Uuid: 1085c1ff-6b79-4021-b6d4-93db50b8709a State: Peer in Cluster ( Connected ) Hostname: swarm3 Uuid: 68525dfd-e91c-4563-9e16-76131025dc68 State: Peer in Cluster ( Connected ) If at this point nodes don't want to connect, there is a high chance that you messed up with the creation of the glusterfs.ca . Create GlusterFS Volumes # We are going to create two separated volumes. One for the different compose files and one for the env volumes. We could have made only one GlusterFS volume for both but I like to separate elements. One thing you have to keep in mind, GlusterFS use bricks in between nodes that are folder. Do not write directly in it but on a mount of it. Compose and env Volumes # We gonna start by creating the brick mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 ssh root@swarm2 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 ssh root@swarm3 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 Create a volume from it. gluster volume create vol_configuration replica 3 transport tcp swarm { 1 ..3 } :/var/glusterfs/no-direct-write-here/vol_configuration/brick1 force volume create: vol_configuration: success: please start the volume to access data Let's activate tcp encryption. gluster volume set vol_configuration client.ssl on volume set: success gluster volume set vol_configuration server.ssl on volume set: success Time to allow our nodes to mount the volume as client and brick data as server. gluster volume set vol_configuration auth.ssl-allow swarm1.lab.local,swarm2.lab.local,swarm3.lab.local volume set: success And now we start it. gluster volume start vol_configuration volume start: vol_configuration: success Now we need to create a mount point for it. mkdir /mnt/configuration_data/ ssh root@swarm2 mkdir /mnt/configuration_data/ ssh root@swarm3 mkdir /mnt/configuration_data/ And now we mount it. mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data ssh root@swarm2 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data ssh root@swarm3 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data We check this is working as it should: And that ssl is activated grep ssl /var/log/glusterfs/mnt-configuration_data.log We want to have it mounted on startup so we add this to /etc/fstab of each node: echo \"localhost:/vol_configuration /mnt/configuration_data glusterfs defaults 0 0\" >> /etc/fstab Docker Data Volumes # Same as above but with the brick made here : /var/glusterfs/no-direct-write-here/vol_docker/brick1 And mountpoint: /mnt/docker_data/ Troubleshoot no GlusterFS mount on startup # I encoutered issue for the glusterFS volume to be mounted as startup as when the fstab is read by the system, glusterd is not yet running. The solution is to translate it in systemd-mount unit. mkdir /etc/systemd/system/glusterfs.mount.d/ ssh root@swarm2 mkdir /etc/systemd/system/configuration_data.mount.d/ ssh root@swarm3 mkdir /etc/systemd/system/configuration_data.mount.d/ Official Documentation : https://docs.gluster.org/en/latest/","title":"Installing GlusterFS Server"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#installing-glusterfs-server","text":"","title":"Installing GlusterFS Server"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#adding-gluster-and-other-repository","text":"dnf install centos-release-gluster dnf install epel-release dnf install dnf-plugins-core dnf config-manager --set-enabled powertools # checking repository list dnf repolist repo id repo name appstream CentOS Stream 8 - AppStream baseos CentOS Stream 8 - BaseOS centos-gluster9 CentOS-8-stream - Gluster 9 epel Extra Packages for Enterprise Linux 8 - x86_64 epel-modular Extra Packages for Enterprise Linux Modular 8 - x86_64 epel-next Extra Packages for Enterprise Linux 8 - Next - x86_64 extras CentOS Stream 8 - Extras powertools CentOS Stream 8 - PowerTools","title":"Adding Gluster and other repository"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#installing-glusterfs-server_1","text":"dnf install glusterfs-server","title":"Installing GlusterFS Server"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#a-bit-of-simplification-for-management","text":"","title":"A bit of simplification for management."},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#generate-ssh-key","text":"ssh-keygen -t ed25519","title":"Generate ssh-key"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#push-ssh-key-to-all-nodes","text":"ssh-copy-id root@swarm2 ssh-copy-id root@swarm3 This should be done on all nodes.","title":"Push ssh-key to all nodes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#stop-firewall","text":"I highly doesn't recommend to stop the firewall but I don't want also that you will get frustrated because of it. So go look how to stop firewalld on CentOS using your favorite search engine. Or read the Firewalling documentation of this website.","title":"Stop firewall"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#configure-glusterfs-with-tls","text":"The fact that we gonna use certificate means you will have to take care to it and specially the expiration date. If certificate expire, GlusterFS service is not going to work. There is way to handle automatic renewall of certificate but this is not going to be part of this article.","title":"Configure GlusterFS with TLS"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#generate-a-private-key-for-each-node","text":"openssl genrsa -out /etc/ssl/glusterfs.key 4096","title":"generate a private key for each node"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#generate-a-signed-certificate-for-each-node","text":"openssl req -new -x509 -key /etc/ssl/glusterfs.key -subj \"/CN=`hostname -f`\" -out /etc/ssl/glusterfs.pem hostname -f : for full fqdn (swarm1.lab.local) hostname -s : for short version (swarm1) This is important to understand what are you going to use on all the nodes as it's going to be used to identify the allowed host to mount Gluster's volume. Oh! Before I forget, it's good to check the expiration date of the certificate :) . By default it expire after 30 days.","title":"generate a signed certificate for each node"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#create-the-glusterca","text":"To be able to use TLS with Gluster Server/Client you need to create /etc/ssl/glusterfs.ca concatenation of others' certificates . The easiest way it's to create a gluster.ca with all three .pem in it and set it up on each nodes. GlusterFS always performs mutual authentication , though clients do not currently do anything with the authenticated server identity. Thus, if client X wants to communicate with server Y, then X's certificate (or that of a signer) must be in Y's CA file, and vice versa. For all uses of TLS in GlusterFS, if one side of a connection is configured to use TLS then the other side must use it as well. There is no automatic fallback to non-TLS communication, or allowance for concurrent TLS and non-TLS access to the same resource, because either would be insecure. Instead, any such \"mixed mode\" connections will be rejected by the TLS-using side, sacrificing availability to maintain security. NOTE : The TLS certificate verification will fail if the machines' date and time are not in sync with each other. Certificate verification depends on the time of the client as well as the server and if that is not found to be in sync then it is deemed to be an invalid certificate. To get the date and times in sync, tools such as ntpdate can be used.","title":"Create the gluster.ca"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#enable-and-start-service","text":"systemctl enable glusterd --now","title":"Enable and start service"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#peer-nodes","text":"From swarm1 run : gluster peer probe swarm2 gluster peer probe swarm3 Verify peers: gluster peer status Number of Peers: 2 Hostname: swarm2 Uuid: 1085c1ff-6b79-4021-b6d4-93db50b8709a State: Peer in Cluster ( Connected ) Hostname: swarm3 Uuid: 68525dfd-e91c-4563-9e16-76131025dc68 State: Peer in Cluster ( Connected ) If at this point nodes don't want to connect, there is a high chance that you messed up with the creation of the glusterfs.ca .","title":"Peer nodes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#create-glusterfs-volumes","text":"We are going to create two separated volumes. One for the different compose files and one for the env volumes. We could have made only one GlusterFS volume for both but I like to separate elements. One thing you have to keep in mind, GlusterFS use bricks in between nodes that are folder. Do not write directly in it but on a mount of it.","title":"Create GlusterFS Volumes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#compose-and-env-volumes","text":"We gonna start by creating the brick mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 ssh root@swarm2 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 ssh root@swarm3 mkdir -p /var/glusterfs/no-direct-write-here/vol_configuration/brick1 Create a volume from it. gluster volume create vol_configuration replica 3 transport tcp swarm { 1 ..3 } :/var/glusterfs/no-direct-write-here/vol_configuration/brick1 force volume create: vol_configuration: success: please start the volume to access data Let's activate tcp encryption. gluster volume set vol_configuration client.ssl on volume set: success gluster volume set vol_configuration server.ssl on volume set: success Time to allow our nodes to mount the volume as client and brick data as server. gluster volume set vol_configuration auth.ssl-allow swarm1.lab.local,swarm2.lab.local,swarm3.lab.local volume set: success And now we start it. gluster volume start vol_configuration volume start: vol_configuration: success Now we need to create a mount point for it. mkdir /mnt/configuration_data/ ssh root@swarm2 mkdir /mnt/configuration_data/ ssh root@swarm3 mkdir /mnt/configuration_data/ And now we mount it. mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data ssh root@swarm2 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data ssh root@swarm3 mount -t glusterfs localhost:/vol_configuration /mnt/configuration_data We check this is working as it should: And that ssl is activated grep ssl /var/log/glusterfs/mnt-configuration_data.log We want to have it mounted on startup so we add this to /etc/fstab of each node: echo \"localhost:/vol_configuration /mnt/configuration_data glusterfs defaults 0 0\" >> /etc/fstab","title":"Compose and env Volumes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#docker-data-volumes","text":"Same as above but with the brick made here : /var/glusterfs/no-direct-write-here/vol_docker/brick1 And mountpoint: /mnt/docker_data/","title":"Docker Data Volumes"},{"location":"3-nodes-swarm/GlusterFS/GlusterFS/#troubleshoot-no-glusterfs-mount-on-startup","text":"I encoutered issue for the glusterFS volume to be mounted as startup as when the fstab is read by the system, glusterd is not yet running. The solution is to translate it in systemd-mount unit. mkdir /etc/systemd/system/glusterfs.mount.d/ ssh root@swarm2 mkdir /etc/systemd/system/configuration_data.mount.d/ ssh root@swarm3 mkdir /etc/systemd/system/configuration_data.mount.d/ Official Documentation : https://docs.gluster.org/en/latest/","title":"Troubleshoot no GlusterFS mount on startup"},{"location":"Stacks/Portainer/","text":"Portainer # Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use. It consists of a single container that can run on any Docker engine (can be deployed as Linux container or a Windows native container, supports other platforms too). Portainer allows you to manage all your Docker resources (containers, images, volumes, networks and more!) It is compatible with the standalone Docker engine and with Docker Swarm mode. Preparaton # Setup Docker Swarm # Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/administration # curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml Customisation # By default portainer stack is using a normal docker volume. So if the node where the container will run fail, then we lost the data. To counter this we are going to use a local file shared accross nodes by GlusterFS. Edit portainer-agent-stack.yml Change: portainer_data:/data for /mnt/docker_data/administration/portainer_data:/data Remove: volumes: portainer_data: The file should look like this (I added some notes as comment): version: '3.2' services: agent: image: portainer/agent volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes networks: - agent_network deploy: mode: global # Will be deploy on all nodes manager or worker accross the swarm. placement: constraints: [node.platform.os == linux] # Will only run on Linux OS. portainer: image: portainer/portainer-ce command: -H tcp://tasks.agent:9001 --tlsskipverify ports: - \"9000:9000\" - \"8000:8000\" volumes: - /mnt/docker_data/administration/portainer_data:/data networks: - agent_network deploy: mode: replicated replicas: 1 # Only one container of this type gonna run at a time. placement: constraints: [node.role == manager] # Will only be deployed on a manager node. networks: agent_network: driver: overlay attachable: true Setup data locations # One issue you gonna have with swarm is that it's not creating folder for declared volume on it's own so you will have to create it by yourself. # mkdir -p /mnt/docker_data/administration/portainer_data If like me you are using the same base directory for docker data, you can run this on the compose file to automatically build the folder. grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. Deploy Portainer stack # Deploy the Portainer stack by running docker stack deploy -c <path -to-docker-compose.yml> portainer Log into your new instance at any nodes on port 9000. You'll be prompted to set your admin user/password on first login. Start at \"Home\", and click on \"Primary\" to manage your swarm (you can manage multiple swarms via one Portainer instance using the agent Todo # Add image Add Some basic usage","title":"Portainer"},{"location":"Stacks/Portainer/#portainer","text":"Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use. It consists of a single container that can run on any Docker engine (can be deployed as Linux container or a Windows native container, supports other platforms too). Portainer allows you to manage all your Docker resources (containers, images, volumes, networks and more!) It is compatible with the standalone Docker engine and with Docker Swarm mode.","title":"Portainer"},{"location":"Stacks/Portainer/#preparaton","text":"","title":"Preparaton"},{"location":"Stacks/Portainer/#setup-docker-swarm","text":"Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/administration # curl -L https://downloads.portainer.io/portainer-agent-stack.yml -o mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml","title":"Setup Docker Swarm"},{"location":"Stacks/Portainer/#customisation","text":"By default portainer stack is using a normal docker volume. So if the node where the container will run fail, then we lost the data. To counter this we are going to use a local file shared accross nodes by GlusterFS. Edit portainer-agent-stack.yml Change: portainer_data:/data for /mnt/docker_data/administration/portainer_data:/data Remove: volumes: portainer_data: The file should look like this (I added some notes as comment): version: '3.2' services: agent: image: portainer/agent volumes: - /var/run/docker.sock:/var/run/docker.sock - /var/lib/docker/volumes:/var/lib/docker/volumes networks: - agent_network deploy: mode: global # Will be deploy on all nodes manager or worker accross the swarm. placement: constraints: [node.platform.os == linux] # Will only run on Linux OS. portainer: image: portainer/portainer-ce command: -H tcp://tasks.agent:9001 --tlsskipverify ports: - \"9000:9000\" - \"8000:8000\" volumes: - /mnt/docker_data/administration/portainer_data:/data networks: - agent_network deploy: mode: replicated replicas: 1 # Only one container of this type gonna run at a time. placement: constraints: [node.role == manager] # Will only be deployed on a manager node. networks: agent_network: driver: overlay attachable: true","title":"Customisation"},{"location":"Stacks/Portainer/#setup-data-locations","text":"One issue you gonna have with swarm is that it's not creating folder for declared volume on it's own so you will have to create it by yourself. # mkdir -p /mnt/docker_data/administration/portainer_data If like me you are using the same base directory for docker data, you can run this on the compose file to automatically build the folder. grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/administration/portainer-agent-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake.","title":"Setup data locations"},{"location":"Stacks/Portainer/#deploy-portainer-stack","text":"Deploy the Portainer stack by running docker stack deploy -c <path -to-docker-compose.yml> portainer Log into your new instance at any nodes on port 9000. You'll be prompted to set your admin user/password on first login. Start at \"Home\", and click on \"Primary\" to manage your swarm (you can manage multiple swarms via one Portainer instance using the agent","title":"Deploy Portainer stack"},{"location":"Stacks/Portainer/#todo","text":"Add image Add Some basic usage","title":"Todo"},{"location":"Stacks/Shuffler/","text":"Shuffle # Shuffle is an Open Source SOAR I really appreciate as it does focus on CyberSecurity. To know more about it go there: https://medium.com/shuffle-automation Or there : Official Website Hi Frikky !!! Hopefully, you will appreciate what you are seing here ^^ Preparaton # Setup data locations # Setup Docker Swarm # Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/SOAR Customisation # At the time of this writing I may be the one of the few shufflying around with swarm :p So there is not yet a stack ready compose file. The swarm mode is still in heavy development but this is fun :) . There is an hidden service shuffle-worker that is created by orborus and there is not yet advanced setup for it. Let's create one /mnt/configuration_data/compose_files/SOAR/shuffle.stack.yml version: '3.8' services: backend: image: ghcr.io/frikky/shuffle-backend:nightly environment: BACKEND_HOSTNAME: backend BACKEND_PORT: '5001' DATASTORE_EMULATOR_HOST: shuffle-database:8000 DB_LOCATION: ./shuffle-database ENVIRONMENT_NAME: Shuffle FRONTEND_PORT: '3001' FRONTEND_PORT_HTTPS: '3443' HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle OUTER_HOSTNAME: backend SHUFFLE_APP_DOWNLOAD_LOCATION: https://github.com/frikky/shuffle-apps SHUFFLE_APP_FORCE_UPDATE: 'false' SHUFFLE_APP_HOTLOAD_FOLDER: /shuffle-apps SHUFFLE_APP_HOTLOAD_LOCATION: ./shuffle-apps SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: '-0.9.30' SHUFFLE_CONTAINER_AUTO_CLEANUP: 'true' SHUFFLE_DEFAULT_APIKEY: '' #SHUFFLE_DEFAULT_PASSWORD: Fr1kky1sN0t@D0g #SHUFFLE_DEFAULT_USERNAME: admin SHUFFLE_DOWNLOAD_AUTH_BRANCH: '' SHUFFLE_DOWNLOAD_AUTH_PASSWORD: '' SHUFFLE_DOWNLOAD_AUTH_USERNAME: '' SHUFFLE_DOWNLOAD_WORKFLOW_BRANCH: '' SHUFFLE_DOWNLOAD_WORKFLOW_LOCATION: '' SHUFFLE_DOWNLOAD_WORKFLOW_PASSWORD: '' SHUFFLE_DOWNLOAD_WORKFLOW_USERNAME: '' SHUFFLE_ELASTIC: 'true' SHUFFLE_FILE_LOCATION: /shuffle-files SHUFFLE_OPENSEARCH_APIKEY: '' SHUFFLE_OPENSEARCH_CERTIFICATE_FILE: '' SHUFFLE_OPENSEARCH_CLOUDID: '' SHUFFLE_OPENSEARCH_PROXY: '' SHUFFLE_OPENSEARCH_SKIPSSL_VERIFY: 'true' SHUFFLE_OPENSEARCH_URL: http://opensearch:9200 SHUFFLE_PASS_APP_PROXY: 'FALSE' SHUFFLE_PASS_WORKER_PROXY: 'TRUE' #SHUFFLE_ENCRYPTION_MODIFIER: #ports: # - 5001:5001 volumes: - /var/run/docker.sock:/var/run/docker.sock - /mnt/docker_data/Shuffle/shuffle-apps:/shuffle-apps - /docker_data/Shuffle/shuffle-files:/shuffle-files networks: - shuffle_prod depends_on: - opensearch logging: driver: json-file deploy: update_config: order: start-first frontend: image: ghcr.io/frikky/shuffle-frontend:nightly healthcheck: test: curl -fs http://localhost:80 || exit 1 interval: 30s timeout: 5s retries: 3 environment: BACKEND_HOSTNAME: backend ports: - 3001:80 - 3443:443 networks: - shuffle_prod logging: driver: json-file depends_on: - backend deploy: update_config: order: start-first opensearch: image: opensearchproject/opensearch:1.2.3 healthcheck: test: curl -fs http://localhost:9200/_cat/health || exit 1 interval: 30s timeout: 5s retries: 3 start_period: 45s environment: OPENSEARCH_JAVA_OPTS: -Xms8192m -Xmx8192m bootstrap.memory_lock: 'false' cluster.initial_master_nodes: opensearch node.store.allow_mmap: 'false' cluster.name: shuffle-cluster cluster.routing.allocation.disk.threshold_enabled: 'false' discovery.seed_hosts: opensearch node.name: opensearch plugins.security.disabled: 'true' #ports: # - 9200:9200 volumes: - /mnt/docker_data/Shuffle/shuffle-database:/usr/share/opensearch/data - /mnt/docker_data/Shuffle/shuffle-backup/opensearch:/mnt/backup networks: - shuffle_prod logging: driver: json-file orborus: image: ghcr.io/frikky/shuffle-orborus:nightly environment: #SHUFFLE_APP_SDK_VERSION: 0.8.97 SHUFFLE_WORKER_VERSION: nightly BASE_URL: http://backend:5001 #BASE_URL: http://phoenix.cogital-it.com:5001 CLEANUP: 'true' DOCKER_API_VERSION: '1.40' ENVIRONMENT_NAME: Shuffle HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: -0.8.80 #SHUFFLE_ORBORUS_EXECUTION_CONCURRENCY: '50' #SHUFFLE_ORBORUS_EXECUTION_TIMEOUT: '800' SHUFFLE_PASS_APP_PROXY: 'FALSE' SHUFFLE_PASS_WORKER_PROXY: 'FALSE' SHUFFLE_SCALE_REPLICAS: 5 SHUFFLE_SWARM_CONFIG: run SHUFFLE_SWARM_NETWORK_NAME: shuffle_prod volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - shuffle_prod depends_on: - backend logging: driver: json-file deploy: update_config: order: start-first networks: shuffle_prod: driver: overlay external: true Setup data locations # This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. # grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/SOAR/shuffle-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' Don't forget to change the elasticsearch data folder permission. chown -R 1000 :1000 /mnt/docker_data/Shuffle/shuffle-database Deploy Shuffle stack # Deploy the Shuffle stack by running docker stack deploy -c <path -to-docker-compose.yml> shuffle Log into your new instance at any nodes on port https://node:3443. Note # SHUFFLE_SCALE_REPLICAS: 5 # Set the number of workers per swarm nodes SHUFFLE_SWARM_CONFIG: run # Set the swarm mode","title":"Shuffle"},{"location":"Stacks/Shuffler/#shuffle","text":"Shuffle is an Open Source SOAR I really appreciate as it does focus on CyberSecurity. To know more about it go there: https://medium.com/shuffle-automation Or there : Official Website Hi Frikky !!! Hopefully, you will appreciate what you are seing here ^^","title":"Shuffle"},{"location":"Stacks/Shuffler/#preparaton","text":"","title":"Preparaton"},{"location":"Stacks/Shuffler/#setup-data-locations","text":"","title":"Setup data locations"},{"location":"Stacks/Shuffler/#setup-docker-swarm","text":"Deploying Portainer and the Portainer Agent to manage a Swarm cluster is easy! You can directly deploy Portainer as a service in your Docker cluster. Note that this method will automatically deploy a single instance of the Portainer Server, and deploy the Portainer Agent as a global service on every node in your cluster. # mkdir -p /mnt/configuration_data/compose_files/SOAR","title":"Setup Docker Swarm"},{"location":"Stacks/Shuffler/#customisation","text":"At the time of this writing I may be the one of the few shufflying around with swarm :p So there is not yet a stack ready compose file. The swarm mode is still in heavy development but this is fun :) . There is an hidden service shuffle-worker that is created by orborus and there is not yet advanced setup for it. Let's create one /mnt/configuration_data/compose_files/SOAR/shuffle.stack.yml version: '3.8' services: backend: image: ghcr.io/frikky/shuffle-backend:nightly environment: BACKEND_HOSTNAME: backend BACKEND_PORT: '5001' DATASTORE_EMULATOR_HOST: shuffle-database:8000 DB_LOCATION: ./shuffle-database ENVIRONMENT_NAME: Shuffle FRONTEND_PORT: '3001' FRONTEND_PORT_HTTPS: '3443' HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle OUTER_HOSTNAME: backend SHUFFLE_APP_DOWNLOAD_LOCATION: https://github.com/frikky/shuffle-apps SHUFFLE_APP_FORCE_UPDATE: 'false' SHUFFLE_APP_HOTLOAD_FOLDER: /shuffle-apps SHUFFLE_APP_HOTLOAD_LOCATION: ./shuffle-apps SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: '-0.9.30' SHUFFLE_CONTAINER_AUTO_CLEANUP: 'true' SHUFFLE_DEFAULT_APIKEY: '' #SHUFFLE_DEFAULT_PASSWORD: Fr1kky1sN0t@D0g #SHUFFLE_DEFAULT_USERNAME: admin SHUFFLE_DOWNLOAD_AUTH_BRANCH: '' SHUFFLE_DOWNLOAD_AUTH_PASSWORD: '' SHUFFLE_DOWNLOAD_AUTH_USERNAME: '' SHUFFLE_DOWNLOAD_WORKFLOW_BRANCH: '' SHUFFLE_DOWNLOAD_WORKFLOW_LOCATION: '' SHUFFLE_DOWNLOAD_WORKFLOW_PASSWORD: '' SHUFFLE_DOWNLOAD_WORKFLOW_USERNAME: '' SHUFFLE_ELASTIC: 'true' SHUFFLE_FILE_LOCATION: /shuffle-files SHUFFLE_OPENSEARCH_APIKEY: '' SHUFFLE_OPENSEARCH_CERTIFICATE_FILE: '' SHUFFLE_OPENSEARCH_CLOUDID: '' SHUFFLE_OPENSEARCH_PROXY: '' SHUFFLE_OPENSEARCH_SKIPSSL_VERIFY: 'true' SHUFFLE_OPENSEARCH_URL: http://opensearch:9200 SHUFFLE_PASS_APP_PROXY: 'FALSE' SHUFFLE_PASS_WORKER_PROXY: 'TRUE' #SHUFFLE_ENCRYPTION_MODIFIER: #ports: # - 5001:5001 volumes: - /var/run/docker.sock:/var/run/docker.sock - /mnt/docker_data/Shuffle/shuffle-apps:/shuffle-apps - /docker_data/Shuffle/shuffle-files:/shuffle-files networks: - shuffle_prod depends_on: - opensearch logging: driver: json-file deploy: update_config: order: start-first frontend: image: ghcr.io/frikky/shuffle-frontend:nightly healthcheck: test: curl -fs http://localhost:80 || exit 1 interval: 30s timeout: 5s retries: 3 environment: BACKEND_HOSTNAME: backend ports: - 3001:80 - 3443:443 networks: - shuffle_prod logging: driver: json-file depends_on: - backend deploy: update_config: order: start-first opensearch: image: opensearchproject/opensearch:1.2.3 healthcheck: test: curl -fs http://localhost:9200/_cat/health || exit 1 interval: 30s timeout: 5s retries: 3 start_period: 45s environment: OPENSEARCH_JAVA_OPTS: -Xms8192m -Xmx8192m bootstrap.memory_lock: 'false' cluster.initial_master_nodes: opensearch node.store.allow_mmap: 'false' cluster.name: shuffle-cluster cluster.routing.allocation.disk.threshold_enabled: 'false' discovery.seed_hosts: opensearch node.name: opensearch plugins.security.disabled: 'true' #ports: # - 9200:9200 volumes: - /mnt/docker_data/Shuffle/shuffle-database:/usr/share/opensearch/data - /mnt/docker_data/Shuffle/shuffle-backup/opensearch:/mnt/backup networks: - shuffle_prod logging: driver: json-file orborus: image: ghcr.io/frikky/shuffle-orborus:nightly environment: #SHUFFLE_APP_SDK_VERSION: 0.8.97 SHUFFLE_WORKER_VERSION: nightly BASE_URL: http://backend:5001 #BASE_URL: http://phoenix.cogital-it.com:5001 CLEANUP: 'true' DOCKER_API_VERSION: '1.40' ENVIRONMENT_NAME: Shuffle HTTPS_PROXY: '' HTTP_PROXY: '' ORG_ID: Shuffle SHUFFLE_BASE_IMAGE_NAME: frikky SHUFFLE_BASE_IMAGE_REGISTRY: ghcr.io SHUFFLE_BASE_IMAGE_TAG_SUFFIX: -0.8.80 #SHUFFLE_ORBORUS_EXECUTION_CONCURRENCY: '50' #SHUFFLE_ORBORUS_EXECUTION_TIMEOUT: '800' SHUFFLE_PASS_APP_PROXY: 'FALSE' SHUFFLE_PASS_WORKER_PROXY: 'FALSE' SHUFFLE_SCALE_REPLICAS: 5 SHUFFLE_SWARM_CONFIG: run SHUFFLE_SWARM_NETWORK_NAME: shuffle_prod volumes: - /var/run/docker.sock:/var/run/docker.sock networks: - shuffle_prod depends_on: - backend logging: driver: json-file deploy: update_config: order: start-first networks: shuffle_prod: driver: overlay external: true","title":"Customisation"},{"location":"Stacks/Shuffler/#setup-data-locations_1","text":"This could be made as a command but I like to run first and echo instead of the \"mkdir -p\" to be sure I didn't made a mistake. # grep \"/mnt/docker_data\" /mnt/configuration_data/compose_files/SOAR/shuffle-stack.yml | awk -F \"- \" '{print $2'} | awk -F \":\" '{ system(\"mkdir -p \"$1\"\") }' Don't forget to change the elasticsearch data folder permission. chown -R 1000 :1000 /mnt/docker_data/Shuffle/shuffle-database","title":"Setup data locations"},{"location":"Stacks/Shuffler/#deploy-shuffle-stack","text":"Deploy the Shuffle stack by running docker stack deploy -c <path -to-docker-compose.yml> shuffle Log into your new instance at any nodes on port https://node:3443.","title":"Deploy Shuffle stack"},{"location":"Stacks/Shuffler/#note","text":"SHUFFLE_SCALE_REPLICAS: 5 # Set the number of workers per swarm nodes SHUFFLE_SWARM_CONFIG: run # Set the swarm mode","title":"Note"}]}